{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpCtYevLHfl4"
   },
   "source": [
    "# **Deep-STORM (2D)**\n",
    "\n",
    "---\n",
    "\n",
    "<font size = 4>Deep-STORM is a neural network capable of image reconstruction from high-density single-molecule localization microscopy (SMLM), first published in 2018 by [Nehme *et al.* in Optica](https://www.osapublishing.org/optica/abstract.cfm?uri=optica-5-4-458). The architecture used here is a U-Net based network without skip connections. This network allows image reconstruction of 2D super-resolution images, in a supervised training manner. The network is trained using simulated high-density SMLM data for which the ground-truth is available. These simulations are obtained from random distribution of single molecules in a field-of-view and therefore do not imprint structural priors during training. The network output a super-resolution image with increased pixel density (typically upsampling factor of 8 in each dimension)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_tjlGzsH-Dn"
   },
   "source": [
    "\n",
    "## **1. Import dependencies & check GPU**\n",
    "---\n",
    "\n",
    "By default, the session should be using Python 3 and GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "before = [str(m) for m in sys.modules]\n",
    "\n",
    "# Import keras modules and libraries\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Activation, UpSampling2D, Convolution2D, MaxPooling2D, BatchNormalization, Layer\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers, losses\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from skimage.transform import warp\n",
    "from skimage.transform import SimilarityTransform\n",
    "from skimage.metrics import structural_similarity\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from scipy.signal import fftconvolve\n",
    "\n",
    "# Import common libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "from os.path import abspath\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import io\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "from PIL import Image \n",
    "from PIL.TiffTags import TAGS\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import math\n",
    "from astropy.visualization import simple_norm\n",
    "from sys import getsizeof\n",
    "from fpdf import FPDF, HTMLMixin\n",
    "from pip._internal.operations.freeze import freeze\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# For sliders and dropdown menu, progress bar\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For Multi-threading in simulation\n",
    "from numba import njit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version is 2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Limit memory usage and print the version of TF if a GPU is detected\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "  print('Tensorflow version is ' + str(tf.__version__))\n",
    "except:\n",
    "  print('Invalid device or cannot modify virtual devices once initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRnQZWSZhArJ"
   },
   "source": [
    "# **2. Code functions**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "id": "kSrZMo3X_NhO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "DeepSTORM installation complete.\n",
      "This notebook is up-to-date.\n"
     ]
    }
   ],
   "source": [
    "Notebook_version = ['1.12']\n",
    "# %% Model definition + helper functions\n",
    "#!pip install fpdf\n",
    "# define a function that projects and rescales an image to the range [0,1]\n",
    "def project_01(im):\n",
    "    im = np.squeeze(im)\n",
    "    min_val = im.min()\n",
    "    max_val = im.max()\n",
    "    return (im - min_val)/(max_val - min_val)\n",
    "\n",
    "# normalize image given mean and std\n",
    "def normalize_im(im, dmean, dstd):\n",
    "    im = np.squeeze(im)\n",
    "    im_norm = np.zeros(im.shape,dtype=np.float32)\n",
    "    im_norm = (im - dmean)/dstd\n",
    "    return im_norm\n",
    "\n",
    "# Define the loss history recorder\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "#  Define a matlab like gaussian 2D filter\n",
    "def matlab_style_gauss2D(shape=(7,7),sigma=1):\n",
    "    \"\"\" \n",
    "    2D gaussian filter - should give the same result as:\n",
    "    MATLAB's fspecial('gaussian',[shape],[sigma]) \n",
    "    \"\"\"\n",
    "    m,n = [(ss-1.)/2. for ss in shape]\n",
    "    y,x = np.ogrid[-m:m+1,-n:n+1]\n",
    "    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n",
    "    h.astype(dtype=K.floatx())\n",
    "    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n",
    "    sumh = h.sum()\n",
    "    if sumh != 0:\n",
    "        h /= sumh\n",
    "    h = h*2.0\n",
    "    h = h.astype('float32')\n",
    "    return h\n",
    "\n",
    "# Expand the filter dimensions\n",
    "psf_heatmap = matlab_style_gauss2D(shape = (7,7),sigma=1)\n",
    "gfilter = tf.reshape(psf_heatmap, [7, 7, 1, 1])\n",
    "\n",
    "# Combined MSE + L1 loss\n",
    "def L1L2loss(input_shape):\n",
    "    def bump_mse(heatmap_true, spikes_pred):\n",
    "\n",
    "        # generate the heatmap corresponding to the predicted spikes\n",
    "        heatmap_pred = K.conv2d(spikes_pred, gfilter, strides=(1, 1), padding='same')\n",
    "\n",
    "        # heatmaps MSE\n",
    "        loss_heatmaps = losses.mean_squared_error(heatmap_true,heatmap_pred)\n",
    "\n",
    "        # l1 on the predicted spikes\n",
    "        loss_spikes = losses.mean_absolute_error(spikes_pred,tf.zeros(input_shape))\n",
    "        return loss_heatmaps + loss_spikes\n",
    "    return bump_mse\n",
    "\n",
    "# Define the concatenated conv2, batch normalization, and relu block\n",
    "def conv_bn_relu(nb_filter, rk, ck, name):\n",
    "    def f(input):\n",
    "        conv = Convolution2D(nb_filter, kernel_size=(rk, ck), strides=(1,1),\\\n",
    "                               padding=\"same\", use_bias=False,\\\n",
    "                               kernel_initializer=\"Orthogonal\",name='conv-'+name)(input)\n",
    "        conv_norm = BatchNormalization(name='BN-'+name)(conv)\n",
    "        conv_norm_relu = Activation(activation = \"relu\",name='Relu-'+name)(conv_norm)\n",
    "        return conv_norm_relu\n",
    "    return f\n",
    "\n",
    "# Define the model architechture\n",
    "def CNN(input,names):\n",
    "    Features1 = conv_bn_relu(32,3,3,names+'F1')(input)\n",
    "    pool1 = MaxPooling2D(pool_size=(2,2),name=names+'Pool1')(Features1)\n",
    "    Features2 = conv_bn_relu(64,3,3,names+'F2')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2),name=names+'Pool2')(Features2)\n",
    "    Features3 = conv_bn_relu(128,3,3,names+'F3')(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2),name=names+'Pool3')(Features3)\n",
    "    Features4 = conv_bn_relu(512,3,3,names+'F4')(pool3)\n",
    "    up5 = UpSampling2D(size=(2, 2),name=names+'Upsample1')(Features4)\n",
    "    Features5 = conv_bn_relu(128,3,3,names+'F5')(up5)\n",
    "    up6 = UpSampling2D(size=(2, 2),name=names+'Upsample2')(Features5)\n",
    "    Features6 = conv_bn_relu(64,3,3,names+'F6')(up6)\n",
    "    up7 = UpSampling2D(size=(2, 2),name=names+'Upsample3')(Features6)\n",
    "    Features7 = conv_bn_relu(32,3,3,names+'F7')(up7)\n",
    "    return Features7\n",
    "\n",
    "# Define the Model building for an arbitrary input size\n",
    "def buildModel(input_dim, initial_learning_rate = 0.001):\n",
    "    input_ = Input (shape = (input_dim))\n",
    "    act_ = CNN (input_,'CNN')\n",
    "    density_pred = Convolution2D(1, kernel_size=(1, 1), strides=(1, 1), padding=\"same\",\\\n",
    "                                  activation=\"linear\", use_bias = False,\\\n",
    "                                  kernel_initializer=\"Orthogonal\",name='Prediction')(act_)\n",
    "    model = Model (inputs= input_, outputs=density_pred)\n",
    "    opt = optimizers.Adam(lr = initial_learning_rate)\n",
    "    model.compile(optimizer=opt, loss = L1L2loss(input_dim))\n",
    "    return model\n",
    "\n",
    "# Define a function that trains a model for a given data SNR and density\n",
    "def train_model(patches, heatmaps, modelPath, epochs, steps_per_epoch, batch_size, upsampling_factor=8, validation_split = 0.3, initial_learning_rate = 0.001, pretrained_model_path = '', L2_weighting_factor = 100):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function trains a CNN model on the desired training set, given the \n",
    "    upsampled training images and labels generated in MATLAB.\n",
    "    \n",
    "    # Inputs\n",
    "    # TO UPDATE ----------\n",
    "\n",
    "    # Outputs\n",
    "    function saves the weights of the trained model to a hdf5, and the \n",
    "    normalization factors to a mat file. These will be loaded later for testing \n",
    "    the model in test_model.    \n",
    "    \"\"\"\n",
    "    \n",
    "    # for reproducibility\n",
    "    np.random.seed(123)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(patches, heatmaps, test_size = validation_split, random_state=42)\n",
    "    print('Number of training examples: %d' % X_train.shape[0])\n",
    "    print('Number of validation examples: %d' % X_test.shape[0])\n",
    "       \n",
    "    # Setting type\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "    y_test = y_test.astype('float32')\n",
    "\n",
    "    \n",
    "    #===================== Training set normalization ==========================\n",
    "    # normalize training images to be in the range [0,1] and calculate the \n",
    "    # training set mean and std\n",
    "    mean_train = np.zeros(X_train.shape[0],dtype=np.float32)\n",
    "    std_train = np.zeros(X_train.shape[0], dtype=np.float32)\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X_train[i, :, :] = project_01(X_train[i, :, :])\n",
    "        mean_train[i] = X_train[i, :, :].mean()\n",
    "        std_train[i] = X_train[i, :, :].std()\n",
    "\n",
    "    # resulting normalized training images\n",
    "    mean_val_train = mean_train.mean()\n",
    "    std_val_train = std_train.mean()\n",
    "    X_train_norm = np.zeros(X_train.shape, dtype=np.float32)\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X_train_norm[i, :, :] = normalize_im(X_train[i, :, :], mean_val_train, std_val_train)\n",
    "    \n",
    "    # patch size\n",
    "    psize = X_train_norm.shape[1]\n",
    "\n",
    "    # Reshaping\n",
    "    X_train_norm = X_train_norm.reshape(X_train.shape[0], psize, psize, 1)\n",
    "\n",
    "    # ===================== Test set normalization ==========================\n",
    "    # normalize test images to be in the range [0,1] and calculate the test set \n",
    "    # mean and std\n",
    "    mean_test = np.zeros(X_test.shape[0],dtype=np.float32)\n",
    "    std_test = np.zeros(X_test.shape[0], dtype=np.float32)\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_test[i, :, :] = project_01(X_test[i, :, :])\n",
    "        mean_test[i] = X_test[i, :, :].mean()\n",
    "        std_test[i] = X_test[i, :, :].std()\n",
    "\n",
    "    # resulting normalized test images\n",
    "    mean_val_test = mean_test.mean()\n",
    "    std_val_test = std_test.mean()\n",
    "    X_test_norm = np.zeros(X_test.shape, dtype=np.float32)\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_test_norm[i, :, :] = normalize_im(X_test[i, :, :], mean_val_test, std_val_test)\n",
    "        \n",
    "    # Reshaping\n",
    "    X_test_norm = X_test_norm.reshape(X_test.shape[0], psize, psize, 1)\n",
    "\n",
    "    # Reshaping labels\n",
    "    Y_train = y_train.reshape(y_train.shape[0], psize, psize, 1)\n",
    "    Y_test = y_test.reshape(y_test.shape[0], psize, psize, 1)\n",
    "\n",
    "    # Save datasets to a matfile to open later in matlab\n",
    "    mdict = {\"mean_test\": mean_val_test, \"std_test\": std_val_test, \"upsampling_factor\": upsampling_factor, \"Normalization factor\": L2_weighting_factor}\n",
    "    sio.savemat(os.path.join(modelPath,\"model_metadata.mat\"), mdict)\n",
    "\n",
    "\n",
    "    # Set the dimensions ordering according to tensorflow consensous\n",
    "    # K.set_image_dim_ordering('tf')\n",
    "    K.set_image_data_format('channels_last')\n",
    "\n",
    "    # Save the model weights after each epoch if the validation loss decreased\n",
    "    checkpointer = ModelCheckpoint(filepath=os.path.join(modelPath,\"weights_best.hdf5\"), verbose=1,\n",
    "                                   save_best_only=True)\n",
    "\n",
    "    # Change learning when loss reaches a plataeu\n",
    "    change_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00005)\n",
    "    \n",
    "    # Model building and complitation\n",
    "    model = buildModel((psize, psize, 1), initial_learning_rate = initial_learning_rate)\n",
    "    model.summary()\n",
    "\n",
    "    # Load pretrained model\n",
    "    if not pretrained_model_path:\n",
    "      print('Using random initial model weights.')\n",
    "    else:\n",
    "      print('Loading model weights from '+pretrained_model_path)\n",
    "      model.load_weights(pretrained_model_path)\n",
    "    \n",
    "    # Create an image data generator for real time data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0.,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.,  # randomly shift images vertically (fraction of total height)\n",
    "        zoom_range=0.,\n",
    "        shear_range=0.,\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        fill_mode='constant',\n",
    "        data_format=K.image_data_format())\n",
    "\n",
    "    # Fit the image generator on the training data\n",
    "    datagen.fit(X_train_norm)\n",
    "    \n",
    "    # loss history recorder\n",
    "    history = LossHistory()\n",
    "\n",
    "    # Inform user training begun\n",
    "    print('-------------------------------')\n",
    "    print('Training model...')\n",
    "\n",
    "    # Fit model on the batches generated by datagen.flow()\n",
    "    train_history = model.fit_generator(datagen.flow(X_train_norm, Y_train, batch_size=batch_size), \n",
    "                                        steps_per_epoch=steps_per_epoch, epochs=epochs, verbose=1, \n",
    "                                        validation_data=(X_test_norm, Y_test), \n",
    "                                        callbacks=[history, checkpointer, change_lr])    \n",
    "\n",
    "    # Inform user training ended\n",
    "    print('-------------------------------')\n",
    "    print('Training Complete!')\n",
    "    \n",
    "    # Save the last model\n",
    "    model.save(os.path.join(modelPath, 'weights_last.hdf5'))\n",
    "\n",
    "    # convert the history.history dict to a pandas DataFrame:     \n",
    "    lossData = pd.DataFrame(train_history.history) \n",
    "\n",
    "    if os.path.exists(os.path.join(modelPath,\"Quality Control\")):\n",
    "      shutil.rmtree(os.path.join(modelPath,\"Quality Control\"))\n",
    "\n",
    "    os.makedirs(os.path.join(modelPath,\"Quality Control\"))\n",
    "\n",
    "    # The training evaluation.csv is saved (overwrites the Files if needed). \n",
    "    lossDataCSVpath = os.path.join(modelPath,\"Quality Control/training_evaluation.csv\")\n",
    "    with open(lossDataCSVpath, 'w') as f:\n",
    "      writer = csv.writer(f)\n",
    "      writer.writerow(['loss','val_loss','learning rate'])\n",
    "      for i in range(len(train_history.history['loss'])):\n",
    "        writer.writerow([train_history.history['loss'][i], train_history.history['val_loss'][i], train_history.history['lr'][i]])\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Normalization functions from Martin Weigert used in CARE\n",
    "def normalize(x, pmin=3, pmax=99.8, axis=None, clip=False, eps=1e-20, dtype=np.float32):\n",
    "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
    "    \"\"\"Percentile-based image normalization.\"\"\"\n",
    "\n",
    "    mi = np.percentile(x,pmin,axis=axis,keepdims=True)\n",
    "    ma = np.percentile(x,pmax,axis=axis,keepdims=True)\n",
    "    return normalize_mi_ma(x, mi, ma, clip=clip, eps=eps, dtype=dtype)\n",
    "\n",
    "\n",
    "def normalize_mi_ma(x, mi, ma, clip=False, eps=1e-20, dtype=np.float32):#dtype=np.float32\n",
    "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
    "    if dtype is not None:\n",
    "        x   = x.astype(dtype,copy=False)\n",
    "        mi  = dtype(mi) if np.isscalar(mi) else mi.astype(dtype,copy=False)\n",
    "        ma  = dtype(ma) if np.isscalar(ma) else ma.astype(dtype,copy=False)\n",
    "        eps = dtype(eps)\n",
    "\n",
    "    try:\n",
    "        import numexpr\n",
    "        x = numexpr.evaluate(\"(x - mi) / ( ma - mi + eps )\")\n",
    "    except ImportError:\n",
    "        x =                   (x - mi) / ( ma - mi + eps )\n",
    "\n",
    "    if clip:\n",
    "        x = np.clip(x,0,1)\n",
    "\n",
    "    return x\n",
    "\n",
    "def norm_minmse(gt, x, normalize_gt=True):\n",
    "    \"\"\"This function is adapted from Martin Weigert\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    normalizes and affinely scales an image pair such that the MSE is minimized  \n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    gt: ndarray\n",
    "        the ground truth image      \n",
    "    x: ndarray\n",
    "        the image that will be affinely scaled \n",
    "    normalize_gt: bool\n",
    "        set to True of gt image should be normalized (default)\n",
    "    Returns\n",
    "    -------\n",
    "    gt_scaled, x_scaled \n",
    "    \"\"\"\n",
    "    if normalize_gt:\n",
    "        gt = normalize(gt, 0.1, 99.9, clip=False).astype(np.float32, copy = False)\n",
    "    x = x.astype(np.float32, copy=False) - np.mean(x)\n",
    "    #x = x - np.mean(x)\n",
    "    gt = gt.astype(np.float32, copy=False) - np.mean(gt)\n",
    "    #gt = gt - np.mean(gt)\n",
    "    scale = np.cov(x.flatten(), gt.flatten())[0, 1] / np.var(x.flatten())\n",
    "    return gt, scale * x\n",
    "\n",
    "\n",
    "# Multi-threaded Erf-based image construction\n",
    "@njit(parallel=True)\n",
    "def FromLoc2Image_Erf(xc_array, yc_array, photon_array, sigma_array, image_size = (64,64), pixel_size = 100):\n",
    "  w = image_size[0]\n",
    "  h = image_size[1]\n",
    "  erfImage = np.zeros((w, h))\n",
    "  for ij in prange(w*h):\n",
    "    j = int(ij/w)\n",
    "    i = ij - j*w\n",
    "    for (xc, yc, photon, sigma) in zip(xc_array, yc_array, photon_array, sigma_array):\n",
    "      # Don't bother if the emitter has photons <= 0 or if Sigma <= 0\n",
    "      if (sigma > 0) and (photon > 0):\n",
    "        S = sigma*math.sqrt(2)\n",
    "        x = i*pixel_size - xc\n",
    "        y = j*pixel_size - yc\n",
    "        # Don't bother if the emitter is further than 4 sigma from the centre of the pixel\n",
    "        if (x+pixel_size/2)**2 + (y+pixel_size/2)**2 < 16*sigma**2:\n",
    "          ErfX = math.erf((x+pixel_size)/S) - math.erf(x/S)\n",
    "          ErfY = math.erf((y+pixel_size)/S) - math.erf(y/S)\n",
    "          erfImage[j][i] += 0.25*photon*ErfX*ErfY\n",
    "  return erfImage\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = (64,64), pixel_size = 100):\n",
    "  w = image_size[0]\n",
    "  h = image_size[1]\n",
    "  locImage = np.zeros((image_size[0],image_size[1]) )\n",
    "  n_locs = len(xc_array)\n",
    "\n",
    "  for e in prange(n_locs):\n",
    "    locImage[int(max(min(round(yc_array[e]/pixel_size),w-1),0))][int(max(min(round(xc_array[e]/pixel_size),h-1),0))] += 1\n",
    "\n",
    "  return locImage\n",
    "\n",
    "\n",
    "def getPixelSizeTIFFmetadata(TIFFpath, display=False):\n",
    "  with Image.open(TIFFpath) as img:\n",
    "    meta_dict = {TAGS[key] : img.tag[key] for key in img.tag.keys()}\n",
    "\n",
    "\n",
    "  # TIFF tags\n",
    "  # https://www.loc.gov/preservation/digital/formats/content/tiff_tags.shtml\n",
    "  # https://www.awaresystems.be/imaging/tiff/tifftags/resolutionunit.html\n",
    "  ResolutionUnit = meta_dict['ResolutionUnit'][0] # unit of resolution\n",
    "  width = meta_dict['ImageWidth'][0]\n",
    "  height = meta_dict['ImageLength'][0]\n",
    "\n",
    "  xResolution = meta_dict['XResolution'][0] # number of pixels / ResolutionUnit\n",
    "\n",
    "  if len(xResolution) == 1:\n",
    "    xResolution = xResolution[0]\n",
    "  elif len(xResolution) == 2:\n",
    "    xResolution = xResolution[0]/xResolution[1]\n",
    "  else:\n",
    "    print('Image resolution not defined.')\n",
    "    xResolution = 1\n",
    "\n",
    "  if ResolutionUnit == 2:\n",
    "    # Units given are in inches\n",
    "    pixel_size = 0.025*1e9/xResolution\n",
    "  elif ResolutionUnit == 3:\n",
    "    # Units given are in cm\n",
    "    pixel_size = 0.01*1e9/xResolution\n",
    "  else: \n",
    "    # ResolutionUnit is therefore 1\n",
    "    print('Resolution unit not defined. Assuming: um')\n",
    "    pixel_size = 1e3/xResolution\n",
    "\n",
    "  if display:\n",
    "    print('Pixel size obtained from metadata: '+str(pixel_size)+' nm')\n",
    "    print('Image size: '+str(width)+'x'+str(height))\n",
    "  \n",
    "  return (pixel_size, width, height)\n",
    "\n",
    "\n",
    "def saveAsTIF(path, filename, array, pixel_size):\n",
    "  \"\"\"\n",
    "  Image saving using PIL to save as .tif format\n",
    "  # Input \n",
    "  path       - path where it will be saved\n",
    "  filename   - name of the file to save (no extension)\n",
    "  array      - numpy array conatining the data at the required format\n",
    "  pixel_size - physical size of pixels in nanometers (identical for x and y)\n",
    "  \"\"\"\n",
    "\n",
    "  # print('Data type: '+str(array.dtype))\n",
    "  if (array.dtype == np.uint16):\n",
    "    mode = 'I;16'\n",
    "  elif (array.dtype == np.uint32):\n",
    "    mode = 'I'\n",
    "  else:\n",
    "    mode = 'F'\n",
    "\n",
    "  # Rounding the pixel size to the nearest number that divides exactly 1cm.\n",
    "  # Resolution needs to be a rational number --> see TIFF format\n",
    "  # pixel_size = 10000/(round(10000/pixel_size))\n",
    "\n",
    "  if len(array.shape) == 2:\n",
    "    im = Image.fromarray(array)\n",
    "    im.save(os.path.join(path, filename+'.tif'),\n",
    "                  mode = mode,  \n",
    "                  resolution_unit = 3,\n",
    "                  resolution = 0.01*1e9/pixel_size)\n",
    "\n",
    "\n",
    "  elif len(array.shape) == 3:\n",
    "    imlist = []\n",
    "    for frame in array:\n",
    "      imlist.append(Image.fromarray(frame))\n",
    "\n",
    "    imlist[0].save(os.path.join(path, filename+'.tif'), save_all=True,\n",
    "                  append_images=imlist[1:],\n",
    "                  mode = mode,  \n",
    "                  resolution_unit = 3,\n",
    "                  resolution = 0.01*1e9/pixel_size)\n",
    "\n",
    "  return\n",
    "\n",
    "\n",
    "class Maximafinder(Layer):\n",
    "    def __init__(self, thresh, neighborhood_size, use_local_avg, **kwargs):\n",
    "        super(Maximafinder, self).__init__(**kwargs)\n",
    "        self.thresh = tf.constant(thresh, dtype=tf.float32)\n",
    "        self.nhood = neighborhood_size\n",
    "        self.use_local_avg = use_local_avg\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.use_local_avg is True:\n",
    "          self.kernel_x = tf.reshape(tf.constant([[-1,0,1],[-1,0,1],[-1,0,1]], dtype=tf.float32), [3, 3, 1, 1])\n",
    "          self.kernel_y = tf.reshape(tf.constant([[-1,-1,-1],[0,0,0],[1,1,1]], dtype=tf.float32), [3, 3, 1, 1])\n",
    "          self.kernel_sum = tf.reshape(tf.constant([[1,1,1],[1,1,1],[1,1,1]], dtype=tf.float32), [3, 3, 1, 1])\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # local maxima positions\n",
    "        max_pool_image = MaxPooling2D(pool_size=(self.nhood,self.nhood), strides=(1,1), padding='same')(inputs)\n",
    "        cond = tf.math.greater(max_pool_image, self.thresh) & tf.math.equal(max_pool_image, inputs)\n",
    "        indices = tf.where(cond)\n",
    "        bind, xind, yind = indices[:, 0], indices[:, 2], indices[:, 1]\n",
    "        confidence = tf.gather_nd(inputs, indices)\n",
    "\n",
    "        # local CoG estimator\n",
    "        if self.use_local_avg:\n",
    "          x_image = K.conv2d(inputs, self.kernel_x, padding='same')\n",
    "          y_image = K.conv2d(inputs, self.kernel_y, padding='same')\n",
    "          sum_image = K.conv2d(inputs, self.kernel_sum, padding='same')\n",
    "          confidence = tf.cast(tf.gather_nd(sum_image, indices), dtype=tf.float32)\n",
    "          x_local = tf.math.divide(tf.gather_nd(x_image, indices),tf.gather_nd(sum_image, indices))\n",
    "          y_local = tf.math.divide(tf.gather_nd(y_image, indices),tf.gather_nd(sum_image, indices))\n",
    "          xind = tf.cast(xind, dtype=tf.float32) + tf.cast(x_local, dtype=tf.float32)\n",
    "          yind = tf.cast(yind, dtype=tf.float32) + tf.cast(y_local, dtype=tf.float32)\n",
    "        else:\n",
    "          xind = tf.cast(xind, dtype=tf.float32)\n",
    "          yind = tf.cast(yind, dtype=tf.float32)\n",
    "        \n",
    "        return bind, xind, yind, confidence\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "        # Implement get_config to enable serialization. This is optional.\n",
    "        base_config = super(Maximafinder, self).get_config()\n",
    "        config = {}\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "# ------------------------------- Prediction with postprocessing  function-------------------------------\n",
    "def batchFramePredictionLocalization(dataPath, filename, modelPath, savePath, batch_size=1, thresh=0.1, neighborhood_size=3, use_local_avg = False, pixel_size = None):\n",
    "    \"\"\"\n",
    "    This function tests a trained model on the desired test set, given the \n",
    "    tiff stack of test images, learned weights, and normalization factors.\n",
    "    \n",
    "    # Inputs\n",
    "    dataPath          - the path to the folder containing the tiff stack(s) to run prediction on \n",
    "    filename          - the name of the file to process\n",
    "    modelPath         - the path to the folder containing the weights file and the mean and standard deviation file generated in train_model\n",
    "    savePath          - the path to the folder where to save the prediction\n",
    "    batch_size.       - the number of frames to predict on for each iteration\n",
    "    thresh            - threshoold percentage from the maximum of the gaussian scaling\n",
    "    neighborhood_size - the size of the neighborhood for local maxima finding\n",
    "    use_local_average - Boolean whether to perform local averaging or not\n",
    "    \"\"\"\n",
    "    \n",
    "    # load mean and std\n",
    "    matfile = sio.loadmat(os.path.join(modelPath,'model_metadata.mat'))\n",
    "    test_mean = np.array(matfile['mean_test'])\n",
    "    test_std = np.array(matfile['std_test'])  \n",
    "    upsampling_factor = np.array(matfile['upsampling_factor'])\n",
    "    upsampling_factor = upsampling_factor.item() # convert to scalar\n",
    "    L2_weighting_factor = np.array(matfile['Normalization factor'])\n",
    "    L2_weighting_factor = L2_weighting_factor.item() # convert to scalar\n",
    "\n",
    "    # Read in the raw file\n",
    "    Images = io.imread(os.path.join(dataPath, filename))\n",
    "    if pixel_size == None:\n",
    "      pixel_size, _, _ = getPixelSizeTIFFmetadata(os.path.join(dataPath, filename), display=True)\n",
    "    pixel_size_hr = pixel_size/upsampling_factor\n",
    "\n",
    "    # get dataset dimensions\n",
    "    (nFrames, M, N) = Images.shape\n",
    "    print('Input image is '+str(N)+'x'+str(M)+' with '+str(nFrames)+' frames.')\n",
    "\n",
    "    # Build the model for a bigger image\n",
    "    model = buildModel((upsampling_factor*M, upsampling_factor*N, 1))\n",
    "\n",
    "    # Load the trained weights\n",
    "    model.load_weights(os.path.join(modelPath,'weights_best.hdf5'))\n",
    "\n",
    "    # add a post-processing module\n",
    "    max_layer = Maximafinder(thresh*L2_weighting_factor, neighborhood_size, use_local_avg)\n",
    "\n",
    "    # Initialise the results: lists will be used to collect all the localizations\n",
    "    frame_number_list, x_nm_list, y_nm_list, confidence_au_list = [], [], [], []\n",
    "\n",
    "    # Initialise the results\n",
    "    Prediction = np.zeros((M*upsampling_factor, N*upsampling_factor), dtype=np.float32)\n",
    "    Widefield = np.zeros((M, N), dtype=np.float32)\n",
    "\n",
    "    # run model in batches\n",
    "    n_batches = math.ceil(nFrames/batch_size)\n",
    "    for b in tqdm(range(n_batches)):\n",
    "\n",
    "      nF = min(batch_size, nFrames - b*batch_size)\n",
    "      Images_norm = np.zeros((nF, M, N),dtype=np.float32)\n",
    "      Images_upsampled = np.zeros((nF, M*upsampling_factor, N*upsampling_factor), dtype=np.float32)\n",
    "\n",
    "      # Upsampling using a simple nearest neighbor interp and calculating - MULTI-THREAD this?\n",
    "      for f in range(nF):\n",
    "        Images_norm[f,:,:] = project_01(Images[b*batch_size+f,:,:])\n",
    "        Images_norm[f,:,:] = normalize_im(Images_norm[f,:,:], test_mean, test_std)\n",
    "        Images_upsampled[f,:,:] = np.kron(Images_norm[f,:,:], np.ones((upsampling_factor,upsampling_factor)))\n",
    "        Widefield += Images[b*batch_size+f,:,:]\n",
    "\n",
    "      # Reshaping\n",
    "      Images_upsampled = np.expand_dims(Images_upsampled,axis=3)\n",
    "\n",
    "      # Run prediction and local amxima finding\n",
    "      predicted_density = model.predict_on_batch(Images_upsampled)\n",
    "      predicted_density[predicted_density < 0] = 0\n",
    "      Prediction += predicted_density.sum(axis = 3).sum(axis = 0)\n",
    "\n",
    "      bind, xind, yind, confidence = max_layer(predicted_density)\n",
    "      \n",
    "      # normalizing the confidence by the L2_weighting_factor\n",
    "      confidence /= L2_weighting_factor \n",
    "\n",
    "      # turn indices to nms and append to the results\n",
    "      xind, yind = xind*pixel_size_hr, yind*pixel_size_hr\n",
    "      frmind = (bind.numpy() + b*batch_size + 1).tolist()\n",
    "      xind = xind.numpy().tolist()\n",
    "      yind = yind.numpy().tolist()\n",
    "      confidence = confidence.numpy().tolist()\n",
    "      frame_number_list += frmind\n",
    "      x_nm_list += xind\n",
    "      y_nm_list += yind\n",
    "      confidence_au_list += confidence\n",
    "\n",
    "    # Open and create the csv file that will contain all the localizations\n",
    "    if use_local_avg:\n",
    "      ext = '_avg'\n",
    "    else:\n",
    "      ext = '_max'\n",
    "    with open(os.path.join(savePath, 'Localizations_' + os.path.splitext(filename)[0] + ext + '.csv'), \"w\", newline='') as file:\n",
    "      writer = csv.writer(file)\n",
    "      writer.writerow(['frame', 'x [nm]', 'y [nm]', 'confidence [a.u]'])\n",
    "      locs = list(zip(frame_number_list, x_nm_list, y_nm_list, confidence_au_list))\n",
    "      writer.writerows(locs)\n",
    "\n",
    "    # Save the prediction and widefield image\n",
    "    Widefield = np.kron(Widefield, np.ones((upsampling_factor,upsampling_factor)))\n",
    "    Widefield = np.float32(Widefield)\n",
    "\n",
    "    # io.imsave(os.path.join(savePath, 'Predicted_'+os.path.splitext(filename)[0]+'.tif'), Prediction)\n",
    "    # io.imsave(os.path.join(savePath, 'Widefield_'+os.path.splitext(filename)[0]+'.tif'), Widefield)\n",
    "    saveAsTIF(savePath, 'Predicted_'+os.path.splitext(filename)[0], Prediction, pixel_size_hr)\n",
    "    saveAsTIF(savePath, 'Widefield_'+os.path.splitext(filename)[0], Widefield, pixel_size_hr)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Colors for the warning messages\n",
    "class bcolors:\n",
    "  WARNING = '\\033[31m'\n",
    "  NORMAL = '\\033[0m'  # white (normal)\n",
    "\n",
    "\n",
    "def list_files(directory, extension):\n",
    "  return (f for f in os.listdir(directory) if f.endswith('.' + extension))\n",
    "\n",
    "\n",
    "# @njit(parallel=True)\n",
    "def subPixelMaxLocalization(array, method = 'CoM', patch_size = 3):\n",
    "  xMaxInd, yMaxInd = np.unravel_index(array.argmax(), array.shape, order='C')\n",
    "  centralPatch = XC[(xMaxInd-patch_size):(xMaxInd+patch_size+1),(yMaxInd-patch_size):(yMaxInd+patch_size+1)]\n",
    "\n",
    "  if (method == 'MAX'):\n",
    "    x0 = xMaxInd\n",
    "    y0 = yMaxInd\n",
    "\n",
    "  elif (method == 'CoM'):\n",
    "    x0 = 0\n",
    "    y0 = 0\n",
    "    S = 0\n",
    "    for xy in range(patch_size*patch_size):\n",
    "      y = math.floor(xy/patch_size)\n",
    "      x = xy - y*patch_size\n",
    "      x0 += x*array[x,y]\n",
    "      y0 += y*array[x,y]\n",
    "      S = array[x,y]\n",
    "    \n",
    "    x0 = x0/S - patch_size/2 + xMaxInd\n",
    "    y0 = y0/S - patch_size/2 + yMaxInd\n",
    "  \n",
    "  elif (method == 'Radiality'):\n",
    "    # Not implemented yet\n",
    "    x0 = xMaxInd\n",
    "    y0 = yMaxInd\n",
    "  \n",
    "  return (x0, y0)\n",
    "\n",
    "\n",
    "@njit(parallel=True)\n",
    "def correctDriftLocalization(xc_array, yc_array, frames, xDrift, yDrift):\n",
    "  n_locs = xc_array.shape[0]\n",
    "  xc_array_Corr = np.empty(n_locs)\n",
    "  yc_array_Corr = np.empty(n_locs)\n",
    "  \n",
    "  for loc in prange(n_locs):\n",
    "    xc_array_Corr[loc] = xc_array[loc] - xDrift[frames[loc]]\n",
    "    yc_array_Corr[loc] = yc_array[loc] - yDrift[frames[loc]]\n",
    "\n",
    "  return (xc_array_Corr, yc_array_Corr)\n",
    "\n",
    "\n",
    "print('--------------------------------')\n",
    "print('DeepSTORM installation complete.')\n",
    "\n",
    "# Check if this is the latest version of the notebook\n",
    "Latest_notebook_version = pd.read_csv(\"https://raw.githubusercontent.com/HenriquesLab/ZeroCostDL4Mic/master/Colab_notebooks/Latest_ZeroCostDL4Mic_Release.csv\")\n",
    "\n",
    "if Notebook_version == list(Latest_notebook_version.columns):\n",
    "  print(\"This notebook is up-to-date.\")\n",
    "\n",
    "if not Notebook_version == list(Latest_notebook_version.columns):\n",
    "  print(bcolors.WARNING +\"A new version of this notebook has been released. We recommend that you download it at https://github.com/HenriquesLab/ZeroCostDL4Mic/wiki\")\n",
    "\n",
    "def pdf_export(trained = False, raw_data = False, pretrained_model = False):\n",
    "  class MyFPDF(FPDF, HTMLMixin):\n",
    "    pass\n",
    "\n",
    "  pdf = MyFPDF()\n",
    "  pdf.add_page()\n",
    "  pdf.set_right_margin(-1)\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B') \n",
    "\n",
    "  Network = 'Deep-STORM'\n",
    "  #model_name = 'little_CARE_test'\n",
    "  day = datetime.now()\n",
    "  datetime_str = str(day)[0:10]\n",
    "\n",
    "  Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n",
    "  pdf.multi_cell(180, 5, txt = Header, align = 'L') \n",
    "    \n",
    "  # add another cell \n",
    "  if trained:\n",
    "    training_time = \"Training time: \"+str(hours)+ \"hour(s) \"+str(minutes)+\"min(s) \"+str(round(seconds))+\"sec(s)\"\n",
    "    pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n",
    "  pdf.ln(1)\n",
    "\n",
    "  Header_2 = 'Information for your materials and method:'\n",
    "  pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n",
    "\n",
    "  all_packages = ''\n",
    "  for requirement in freeze(local_only=True):\n",
    "    all_packages = all_packages+requirement+', '\n",
    "  #print(all_packages)\n",
    "\n",
    "  #Main Packages\n",
    "  main_packages = ''\n",
    "  version_numbers = []\n",
    "  for name in ['tensorflow','numpy','Keras']:\n",
    "    find_name=all_packages.find(name)\n",
    "    main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n",
    "    #Version numbers only here:\n",
    "    version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n",
    "\n",
    "  cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n",
    "  cuda_version = cuda_version.stdout.decode('utf-8')\n",
    "  cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n",
    "  gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n",
    "  gpu_name = gpu_name.stdout.decode('utf-8')\n",
    "  gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n",
    "  #print(cuda_version[cuda_version.find(', V')+3:-1])\n",
    "  #print(gpu_name)\n",
    "  if raw_data == True:\n",
    "    shape = (M,N)\n",
    "  else:\n",
    "    shape = (int(FOV_size/pixel_size),int(FOV_size/pixel_size))\n",
    "  #dataset_size = len(os.listdir(Training_source))\n",
    "\n",
    "  text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(n_patches)+' paired image patches (image dimensions: '+str(patch_size)+', patch size (upsampled): ('+str(int(patch_size))+','+str(int(patch_size))+') with a batch size of '+str(batch_size)+', using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Losses were calculated using MSE for the heatmaps and L1 loss for the spike prediction. Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), Keras (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+' GPU.'\n",
    "\n",
    "  if pretrained_model:\n",
    "    text = 'The '+Network+' model was trained from scratch for '+str(number_of_epochs)+' epochs on '+str(n_patches)+' paired image patches (image dimensions: '+str(patch_size)+', patch size (upsampled): ('+str(int(patch_size))+','+str(int(patch_size))+') with a batch size of '+str(batch_size)+', using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Losses were calculated using MSE for the heatmaps and L1 loss for the spike prediction. The models was retrained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), numpy (v '+version_numbers[1]+'), Keras (v '+version_numbers[2]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+' GPU.'\n",
    "\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  pdf.multi_cell(180, 5, txt = text, align='L')\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(190, 5, txt = 'Training dataset', align='L', ln=1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  if raw_data==False:\n",
    "    simul_text = 'The training dataset was created in the notebook using the following simulation settings:'\n",
    "    pdf.cell(200, 5, txt=simul_text, align='L')\n",
    "    pdf.ln(1)\n",
    "    html = \"\"\" \n",
    "    <table width=60% style=\"margin-left:0px;\">\n",
    "      <tr>\n",
    "        <th width = 50% align=\"left\">Setting</th>\n",
    "        <th width = 50% align=\"left\">Simulated Value</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>FOV_size</td>\n",
    "        <td width = 50%>{0}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>pixel_size</td>\n",
    "        <td width = 50%>{1}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>ADC_per_photon_conversion</td>\n",
    "        <td width = 50%>{2}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>ReadOutNoise_ADC</td>\n",
    "        <td width = 50%>{3}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>ADC_offset</td>\n",
    "        <td width = 50%>{4}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>emitter_density</td>\n",
    "        <td width = 50%>{5}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>emitter_density_std</td>\n",
    "        <td width = 50%>{6}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>number_of_frames</td>\n",
    "        <td width = 50%>{7}</td>\n",
    "      </tr> \n",
    "      <tr>\n",
    "        <td width = 50%>sigma</td>\n",
    "        <td width = 50%>{8}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>sigma_std</td>\n",
    "        <td width = 50%>{9}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>n_photons</td>\n",
    "        <td width = 50%>{10}</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td width = 50%>n_photons_std</td>\n",
    "        <td width = 50%>{11}</td>\n",
    "      </tr> \n",
    "    </table>\n",
    "    \"\"\".format(FOV_size, pixel_size, ADC_per_photon_conversion, ReadOutNoise_ADC, ADC_offset, emitter_density, emitter_density_std, number_of_frames, sigma, sigma_std, n_photons, n_photons_std)\n",
    "    pdf.write_html(html)\n",
    "  else:\n",
    "    simul_text = 'The training dataset was simulated using ThunderSTORM and loaded into the notebook.'\n",
    "    pdf.multi_cell(190, 5, txt=simul_text, align='L')\n",
    "    pdf.set_font(\"Arial\", size = 11, style='B')\n",
    "    #pdf.ln(1)\n",
    "    #pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n",
    "    pdf.set_font('')\n",
    "    pdf.set_font('Arial', size = 10, style = 'B')\n",
    "    pdf.cell(29, 5, txt= 'ImageData_path', align = 'L', ln=0)\n",
    "    pdf.set_font('')\n",
    "    pdf.multi_cell(170, 5, txt = ImageData_path, align = 'L')\n",
    "    pdf.set_font('')\n",
    "    pdf.set_font('Arial', size = 10, style = 'B')\n",
    "    pdf.cell(28, 5, txt= 'LocalizationData_path:', align = 'L', ln=0)\n",
    "    pdf.set_font('')\n",
    "    pdf.multi_cell(170, 5, txt = LocalizationData_path, align = 'L')\n",
    "    pdf.set_font('Arial', size = 10, style = 'B')\n",
    "    pdf.cell(28, 5, txt= 'pixel_size:', align = 'L', ln=0)\n",
    "    pdf.set_font('')\n",
    "    pdf.multi_cell(170, 5, txt = str(pixel_size), align = 'L')\n",
    "  #pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  # if Use_Default_Advanced_Parameters:\n",
    "  #   pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n",
    "  pdf.cell(200, 5, txt='The following parameters were used to generate patches:')\n",
    "  pdf.ln(1)\n",
    "  html = \"\"\"\n",
    "  <table width=70% style=\"margin-left:0px;\">\n",
    "    <tr>\n",
    "      <th width = 50% align=\"left\">Patch Parameter</th>\n",
    "      <th width = 50% align=\"left\">Value</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>patch_size</td>\n",
    "      <td width = 50%>{0}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>upsampling_factor</td>\n",
    "      <td width = 50%>{1}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>num_patches_per_frame</td>\n",
    "      <td width = 50%>{2}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>min_number_of_emitters_per_patch</td>\n",
    "      <td width = 50%>{3}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>max_num_patches</td>\n",
    "      <td width = 50%>{4}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>gaussian_sigma</td>\n",
    "      <td width = 50%>{5}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>Automatic_normalization</td>\n",
    "      <td width = 50%>{6}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>L2_weighting_factor</td>\n",
    "      <td width = 50%>{7}</td>\n",
    "    </tr>\n",
    "  \"\"\".format(str(patch_size)+'x'+str(patch_size), upsampling_factor, num_patches_per_frame, min_number_of_emitters_per_patch, max_num_patches, gaussian_sigma, Automatic_normalization, L2_weighting_factor)\n",
    "  pdf.write_html(html)\n",
    "  pdf.ln(3)\n",
    "  pdf.set_font('Arial', size=10)\n",
    "  pdf.cell(200, 5, txt='The following parameters were used for training:')\n",
    "  pdf.ln(1)\n",
    "  html = \"\"\" \n",
    "  <table width=70% style=\"margin-left:0px;\">\n",
    "    <tr>\n",
    "      <th width = 50% align=\"left\">Training Parameter</th>\n",
    "      <th width = 50% align=\"left\">Value</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>number_of_epochs</td>\n",
    "      <td width = 50%>{0}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>batch_size</td>\n",
    "      <td width = 50%>{1}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>number_of_steps</td>\n",
    "      <td width = 50%>{2}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>percentage_validation</td>\n",
    "      <td width = 50%>{3}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td width = 50%>initial_learning_rate</td>\n",
    "      <td width = 50%>{4}</td>\n",
    "    </tr>\n",
    "  </table>\n",
    "  \"\"\".format(number_of_epochs,batch_size,number_of_steps,percentage_validation,initial_learning_rate)\n",
    "  pdf.write_html(html)\n",
    "\n",
    "  pdf.ln(1)\n",
    "  # pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.cell(21, 5, txt= 'Model Path:', align = 'L', ln=0)\n",
    "  pdf.set_font('')\n",
    "  pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n",
    "\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(60, 5, txt = 'Example Training Images', ln=1)\n",
    "  pdf.ln(1)\n",
    "  exp_size = io.imread(path_to_fig).shape # path_to_fig\n",
    "  pdf.image(path_to_fig, x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
    "  pdf.ln(1)\n",
    "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy.\" bioRxiv (2020).'\n",
    "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
    "  ref_2 = '- Deep-STORM: Nehme, Elias, et al. \"Deep-STORM: super-resolution single-molecule microscopy by deep learning.\" Optica 5.4 (2018): 458-464.'\n",
    "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
    "  # if Use_Data_augmentation:\n",
    "  #   ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n",
    "  #   pdf.multi_cell(190, 5, txt = ref_3, align='L')\n",
    "  pdf.ln(3)\n",
    "  reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n",
    "  pdf.set_font('Arial', size = 11, style='B')\n",
    "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
    "\n",
    "  pdf.output(model_path+'/'+model_name+'/'+model_name+'_training_report.pdf')\n",
    "  print('------------------------------')\n",
    "  print('PDF report exported in '+model_path+'/'+model_name+'/')\n",
    "\n",
    "def qc_pdf_export():\n",
    "  class MyFPDF(FPDF, HTMLMixin):\n",
    "    pass\n",
    "\n",
    "  pdf = MyFPDF()\n",
    "  pdf.add_page()\n",
    "  pdf.set_right_margin(-1)\n",
    "  pdf.set_font(\"Arial\", size = 11, style='B') \n",
    "\n",
    "  Network = 'Deep-STORM'\n",
    "  #model_name = os.path.basename(full_QC_model_path)\n",
    "  day = datetime.now()\n",
    "  datetime_str = str(day)[0:10]\n",
    "\n",
    "  Header = 'Quality Control report for '+Network+' model ('+os.path.basename(QC_model_path)+')\\nDate: '+datetime_str\n",
    "  pdf.multi_cell(180, 5, txt = Header, align = 'L') \n",
    "\n",
    "  all_packages = ''\n",
    "  for requirement in freeze(local_only=True):\n",
    "    all_packages = all_packages+requirement+', '\n",
    "\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(2)\n",
    "  pdf.cell(190, 5, txt = 'Loss curves', ln=1, align='L')\n",
    "  pdf.ln(1)\n",
    "  if os.path.exists(savePath+'/lossCurvePlots.png'):\n",
    "    exp_size = io.imread(savePath+'/lossCurvePlots.png').shape\n",
    "    pdf.image(savePath+'/lossCurvePlots.png', x = 11, y = None, w = round(exp_size[1]/10), h = round(exp_size[0]/10))\n",
    "  else:\n",
    "    pdf.set_font('')\n",
    "    pdf.set_font('Arial', size=10)\n",
    "    pdf.cell(190, 5, txt='If you would like to see the evolution of the loss function during training please play the first cell of the QC section in the notebook.')\n",
    "  pdf.ln(2)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 10, style = 'B')\n",
    "  pdf.ln(3)\n",
    "  pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n",
    "  pdf.ln(1)\n",
    "  exp_size = io.imread(savePath+'/QC_example_data.png').shape\n",
    "  pdf.image(savePath+'/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font('Arial', size = 11, style = 'B')\n",
    "  pdf.ln(1)\n",
    "  pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "\n",
    "  pdf.ln(1)\n",
    "  html = \"\"\"\n",
    "  <body>\n",
    "  <font size=\"7\" face=\"Courier New\" >\n",
    "  <table width=94% style=\"margin-left:0px;\">\"\"\"\n",
    "  with open(savePath+'/'+os.path.basename(QC_model_path)+'_QC_metrics.csv', 'r') as csvfile:\n",
    "    metrics = csv.reader(csvfile)\n",
    "    header = next(metrics)\n",
    "    image = header[0]\n",
    "    mSSIM_PvsGT = header[1]\n",
    "    mSSIM_SvsGT = header[2]\n",
    "    NRMSE_PvsGT = header[3]\n",
    "    NRMSE_SvsGT = header[4]\n",
    "    PSNR_PvsGT = header[5]\n",
    "    PSNR_SvsGT = header[6]\n",
    "    header = \"\"\"\n",
    "    <tr>\n",
    "    <th width = 10% align=\"left\">{0}</th>\n",
    "    <th width = 15% align=\"left\">{1}</th>\n",
    "    <th width = 15% align=\"center\">{2}</th>\n",
    "    <th width = 15% align=\"left\">{3}</th>\n",
    "    <th width = 15% align=\"center\">{4}</th>\n",
    "    <th width = 15% align=\"left\">{5}</th>\n",
    "    <th width = 15% align=\"center\">{6}</th>\n",
    "    </tr>\"\"\".format(image,mSSIM_PvsGT,mSSIM_SvsGT,NRMSE_PvsGT,NRMSE_SvsGT,PSNR_PvsGT,PSNR_SvsGT)\n",
    "    html = html+header\n",
    "    for row in metrics:\n",
    "      image = row[0]\n",
    "      mSSIM_PvsGT = row[1]\n",
    "      mSSIM_SvsGT = row[2]\n",
    "      NRMSE_PvsGT = row[3]\n",
    "      NRMSE_SvsGT = row[4]\n",
    "      PSNR_PvsGT = row[5]\n",
    "      PSNR_SvsGT = row[6]\n",
    "      cells = \"\"\"\n",
    "        <tr>\n",
    "          <td width = 10% align=\"left\">{0}</td>\n",
    "          <td width = 15% align=\"center\">{1}</td>\n",
    "          <td width = 15% align=\"center\">{2}</td>\n",
    "          <td width = 15% align=\"center\">{3}</td>\n",
    "          <td width = 15% align=\"center\">{4}</td>\n",
    "          <td width = 15% align=\"center\">{5}</td>\n",
    "          <td width = 15% align=\"center\">{6}</td>\n",
    "        </tr>\"\"\".format(image,str(round(float(mSSIM_PvsGT),3)),str(round(float(mSSIM_SvsGT),3)),str(round(float(NRMSE_PvsGT),3)),str(round(float(NRMSE_SvsGT),3)),str(round(float(PSNR_PvsGT),3)),str(round(float(PSNR_SvsGT),3)))\n",
    "      html = html+cells\n",
    "    html = html+\"\"\"</body></table>\"\"\"\n",
    "    \n",
    "  pdf.write_html(html)\n",
    "\n",
    "  pdf.ln(1)\n",
    "  pdf.set_font('')\n",
    "  pdf.set_font_size(10.)\n",
    "  ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"ZeroCostDL4Mic: an open platform to simplify access and use of Deep-Learning in Microscopy.\" bioRxiv (2020).'\n",
    "  pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
    "  ref_2 = '- Deep-STORM: Nehme, Elias, et al. \"Deep-STORM: super-resolution single-molecule microscopy by deep learning.\" Optica 5.4 (2018): 458-464.'\n",
    "  pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
    "\n",
    "  pdf.ln(3)\n",
    "  reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n",
    "\n",
    "  pdf.set_font('Arial', size = 11, style='B')\n",
    "  pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
    "\n",
    "  pdf.output(savePath+'/'+os.path.basename(QC_model_path)+'_QC_report.pdf')\n",
    "\n",
    "\n",
    "  print('------------------------------')\n",
    "  print('QC PDF report exported as '+savePath+'/'+os.path.basename(QC_model_path)+'_QC_report.pdf')\n",
    "\n",
    "\n",
    "\n",
    "# Exporting requirements.txt for local run\n",
    "# !pip freeze > requirements.txt\n",
    "\n",
    "after = [str(m) for m in sys.modules]\n",
    "# Get minimum requirements file\n",
    "\n",
    "#Add the following lines before all imports: \n",
    "#  import sys\n",
    "#  before = [str(m) for m in sys.modules]\n",
    "\n",
    "#Add the following line after the imports:\n",
    "#  after = [str(m) for m in sys.modules]\n",
    "\n",
    "from builtins import any as b_any\n",
    "\n",
    "def filter_files(file_list, filter_list):\n",
    "    filtered_list = []\n",
    "    for fname in file_list:\n",
    "        if b_any(fname.split('==')[0] in s for s in filter_list):\n",
    "            filtered_list.append(fname)\n",
    "    return filtered_list\n",
    "\n",
    "df = pd.read_csv('requirements.txt', delimiter = \"\\n\")\n",
    "mod_list = [m.split('.')[0] for m in after if not m in before]\n",
    "\n",
    "req_list_temp = df.values.tolist()\n",
    "req_list = [x[0] for x in req_list_temp]\n",
    "\n",
    "# Replace with package name \n",
    "mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n",
    "mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list] \n",
    "filtered_list = filter_files(req_list, mod_replace_list)\n",
    "\n",
    "file=open('DeepSTORM_2D_requirements_simple.txt','w')\n",
    "for item in filtered_list:\n",
    "    file.writelines(item + '\\n')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu8f5NGJkJos"
   },
   "source": [
    "\n",
    "# **3. Generate patches for training**\n",
    "---\n",
    "\n",
    "For Deep-STORM the training data can be obtained in two ways:\n",
    "* Simulated using ThunderSTORM or other simulation tool and loaded here (**using Section 3.1.a**)\n",
    "* Directly simulated in this notebook (**using Section 3.1.b**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSV8xnlynp0l"
   },
   "source": [
    "## **3.1.a Load training data**\n",
    "---\n",
    "\n",
    "Here you can load your simulated data along with its corresponding localization file.\n",
    "*   The `pixel_size` is defined in nanometer (nm). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "CT6SNcfNg6j0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images: 512x512 with 1001 frames\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03da0aeebefa4634855e5e8ba0b5c69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, continuous_update=False, description='frame', max=1001, min=1), Outpu"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x [nm]</th>\n",
       "      <th>y [nm]</th>\n",
       "      <th>Photon #</th>\n",
       "      <th>Sigma [nm]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>233091</th>\n",
       "      <td>1001</td>\n",
       "      <td>53307.88</td>\n",
       "      <td>45758.62</td>\n",
       "      <td>8563</td>\n",
       "      <td>133.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233092</th>\n",
       "      <td>1001</td>\n",
       "      <td>53949.00</td>\n",
       "      <td>44747.32</td>\n",
       "      <td>10080</td>\n",
       "      <td>129.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233093</th>\n",
       "      <td>1001</td>\n",
       "      <td>54113.51</td>\n",
       "      <td>18785.53</td>\n",
       "      <td>847</td>\n",
       "      <td>129.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233094</th>\n",
       "      <td>1001</td>\n",
       "      <td>54141.98</td>\n",
       "      <td>25588.86</td>\n",
       "      <td>3282</td>\n",
       "      <td>129.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233095</th>\n",
       "      <td>1001</td>\n",
       "      <td>54191.56</td>\n",
       "      <td>12081.65</td>\n",
       "      <td>14215</td>\n",
       "      <td>116.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        frame    x [nm]    y [nm]  Photon #  Sigma [nm]\n",
       "233091   1001  53307.88  45758.62      8563      133.22\n",
       "233092   1001  53949.00  44747.32     10080      129.84\n",
       "233093   1001  54113.51  18785.53       847      129.49\n",
       "233094   1001  54141.98  25588.86      3282      129.58\n",
       "233095   1001  54191.56  12081.65     14215      116.80"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@markdown ##Load raw data\n",
    "\n",
    "load_raw_data = True\n",
    "\n",
    "######################################################################\n",
    "# ---------------------------- User input ----------------------------\n",
    "ImageData_path = \"D:\\Data\\Anatomy\\KaarjelNara\\\\2021-03-10\\\\A.tif\" #@param {type:\"string\"}\n",
    "LocalizationData_path = \"D:\\Data\\Anatomy\\KaarjelNara\\\\2021-03-10\\\\A.csv\" #@param {type: \"string\"}\n",
    "#@markdown Get pixel size from file?\n",
    "get_pixel_size_from_file = False #@param {type:\"boolean\"} False\n",
    "#@markdown Otherwise, use this value:\n",
    "pixel_size = 107 #@param {type:\"number\"}\n",
    "######################################################################\n",
    "\n",
    "if get_pixel_size_from_file:\n",
    "  pixel_size,_,_ = getPixelSizeTIFFmetadata(ImageData_path, True)\n",
    "\n",
    "# load the tiff data\n",
    "Images = io.imread(ImageData_path)\n",
    "# get dataset dimensions\n",
    "if len(Images.shape) == 3:\n",
    "  (number_of_frames, M, N) = Images.shape\n",
    "elif len(Images.shape) == 2:\n",
    "  (M, N) = Images.shape\n",
    "  number_of_frames = 1\n",
    "print('Loaded images: '+str(M)+'x'+str(N)+' with '+str(number_of_frames)+' frames')\n",
    "\n",
    "# Interactive display of the stack\n",
    "def scroll_in_time(frame):\n",
    "    f=plt.figure(figsize=(6,6))\n",
    "    plt.imshow(Images[frame-1], interpolation='nearest', cmap = 'gray')\n",
    "    plt.title('Training source at frame = ' + str(frame))\n",
    "    plt.axis('off');\n",
    "\n",
    "if number_of_frames > 1:\n",
    "  interact(scroll_in_time, frame=widgets.IntSlider(min=1, max=Images.shape[0], step=1, value=0, continuous_update=False));\n",
    "else:\n",
    "  f=plt.figure(figsize=(6,6))\n",
    "  plt.imshow(Images, interpolation='nearest', cmap = 'gray')\n",
    "  plt.title('Training source')\n",
    "  plt.axis('off');\n",
    "\n",
    "# Load the localization file and display the first\n",
    "LocData = pd.read_csv(LocalizationData_path, index_col=0)\n",
    "LocData.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_8e3kE-JhVY"
   },
   "source": [
    "## **3.2. Generate training patches**\n",
    "---\n",
    "\n",
    "Training patches need to be created from the training data generated above. \n",
    "*   The `patch_size` needs to give sufficient contextual information and for most cases a `patch_size` of 26 (corresponding to patches of 26x26 pixels) works fine. **DEFAULT: 26**\n",
    "*   The `upsampling_factor` defines the effective magnification of the final super-resolved image compared to the input image (this is called magnification in ThunderSTORM). This is used to generate the super-resolved patches as target dataset. Using an `upsampling_factor` of 16 will require the use of more memory and it may be necessary to decreae the `patch_size` to 16 for example. **DEFAULT: 8**\n",
    "*   The `num_patches_per_frame` defines the number of patches extracted from each frame generated in section 3.1. **DEFAULT: 500**\n",
    "*   The `min_number_of_emitters_per_patch` defines the minimum number of emitters that need to be present in the patch to be a valid patch. An empty patch does not contain useful information for the network to learn from. **DEFAULT: 7**\n",
    "*   The `max_num_patches` defines the maximum number of patches to generate. Fewer may be generated depending on how many pacthes are rejected and how many frames are available. **DEFAULT: 10000**\n",
    "*   The `gaussian_sigma` defines the Gaussian standard deviation (in magnified pixels) applied to generate the super-resolved target image. **DEFAULT: 1**\n",
    "*   The `L2_weighting_factor` is a normalization factor used in the loss function. It helps balancing the loss from the L2 norm. When using higher densities, this factor should be decreased and vice-versa. This factor can be autimatically calculated using an empiraical formula. **DEFAULT: 100**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "AsNx5KzcFNvC",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/1001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 patches of 208x208\n",
      "Total number of localizations: 233095\n",
      "Density: 0.08 locs/um^2\n",
      "Normalization factor: 450.33\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1001/1001 [11:58<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches skipped due to low density: 13011\n",
      "10000 patches were generated.\n",
      "Time elapsed: 0.0 hour(s) 11.0 min(s) 58 sec(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df28c2e86844437395b03ee42b3e2acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, continuous_update=False, description='patch', max=10000, min=1), Outp"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown ## **Provide patch parameters**\n",
    "\n",
    "######################################################################\n",
    "# ---------------------------- User input ----------------------------\n",
    "patch_size = 26 #@param {type:\"integer\"}\n",
    "upsampling_factor = 8 #@param [\"4\", \"8\", \"16\"] {type:\"raw\"}\n",
    "num_patches_per_frame =  500#@param {type:\"integer\"}\n",
    "min_number_of_emitters_per_patch = 1#@param {type:\"integer\"}\n",
    "max_num_patches =  10000#@param {type:\"integer\"}\n",
    "gaussian_sigma = 1#@param {type:\"integer\"}\n",
    "\n",
    "#@markdown Estimate the optimal normalization factor automatically?\n",
    "Automatic_normalization = True #@param {type:\"boolean\"}\n",
    "#@markdown Otherwise, it will use the following value:\n",
    "L2_weighting_factor = 100 #@param {type:\"number\"}\n",
    "\n",
    "# Save plot with fig name \"D:\\Data\\Anatomy\\KaarjelNara\\MyFig.png\"\n",
    "path_to_fig = \"D:\\Data\\Anatomy\\KaarjelNara\\TrainingDataExample_DeepSTORM2D.png\"\n",
    "######################################################################\n",
    "\n",
    "\n",
    "# -------------------- Prepare variables --------------------\n",
    "# Start the clock to measure how long it takes\n",
    "start = time.time()\n",
    "\n",
    "# Initialize some parameters\n",
    "pixel_size_hr = pixel_size/upsampling_factor # in nm\n",
    "n_patches = min(number_of_frames*num_patches_per_frame, max_num_patches)\n",
    "patch_size = patch_size*upsampling_factor\n",
    "\n",
    "# Dimensions of the high-res grid\n",
    "Mhr = upsampling_factor*M # in pixels\n",
    "Nhr = upsampling_factor*N # in pixels\n",
    "\n",
    "# Initialize the training patches and labels\n",
    "patches = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n",
    "spikes = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n",
    "heatmaps = np.zeros((n_patches, patch_size, patch_size), dtype = np.float32)\n",
    "\n",
    "# Run over all frames and construct the training examples\n",
    "k = 1 # current patch count\n",
    "skip_counter = 0 # number of dataset skipped due to low density\n",
    "id_start = 0 # id position in LocData for current frame\n",
    "print('Generating '+str(n_patches)+' patches of '+str(patch_size)+'x'+str(patch_size))\n",
    "\n",
    "n_locs = len(LocData.index)\n",
    "print('Total number of localizations: '+str(n_locs))\n",
    "density = n_locs/(M*N*number_of_frames*(0.001*pixel_size)**2)\n",
    "print('Density: '+str(round(density,2))+' locs/um^2')\n",
    "n_locs_per_patch = patch_size**2*density\n",
    "\n",
    "if Automatic_normalization:\n",
    "  # This empirical formulae attempts to balance the loss L2 function between the background and the bright spikes\n",
    "  # A value of 100 was originally chosen to balance L2 for a patch size of 2.6x2.6^2 0.1um pixel size and density of 3 (hence the 20.28), at upsampling_factor = 8\n",
    "  L2_weighting_factor = 100/math.sqrt(min(n_locs_per_patch, min_number_of_emitters_per_patch)*8**2/(upsampling_factor**2*20.28))\n",
    "  print('Normalization factor: '+str(round(L2_weighting_factor,2)))\n",
    "\n",
    "    \n",
    "# -------------------- Patch generation loop --------------------\n",
    "print('-----------------------------------------------------------')\n",
    "for (f, thisFrame) in enumerate(tqdm(Images)):\n",
    "\n",
    "  # Upsample the frame\n",
    "  upsampledFrame = np.kron(thisFrame, np.ones((upsampling_factor,upsampling_factor)))\n",
    "  # Read all the provided high-resolution locations for current frame\n",
    "  DataFrame = LocData[LocData['frame'] == f+1].copy()\n",
    "\n",
    "  # Get the approximated locations according to the high-res grid pixel size\n",
    "  Chr_emitters = [int(max(min(round(DataFrame['x [nm]'][i]/pixel_size_hr),Nhr-1),0)) for i in range(id_start+1,id_start+1+len(DataFrame.index))]\n",
    "  Rhr_emitters = [int(max(min(round(DataFrame['y [nm]'][i]/pixel_size_hr),Mhr-1),0)) for i in range(id_start+1,id_start+1+len(DataFrame.index))]\n",
    "  id_start += len(DataFrame.index)\n",
    "\n",
    "  # Build Localization image\n",
    "  LocImage = np.zeros((Mhr,Nhr))\n",
    "  LocImage[(Rhr_emitters, Chr_emitters)] = 1\n",
    "\n",
    "  # Here, there's a choice between the original Gaussian (classification approach) and using the erf function\n",
    "  HeatMapImage = L2_weighting_factor*gaussian_filter(LocImage, gaussian_sigma)  \n",
    "  # HeatMapImage = L2_weighting_factor*FromLoc2Image_MultiThreaded(np.array(list(DataFrame['x [nm]'])), np.array(list(DataFrame['y [nm]'])), \n",
    "                                                            #  np.ones(len(DataFrame.index)), pixel_size_hr*gaussian_sigma*np.ones(len(DataFrame.index)), \n",
    "                                                            #  Mhr, pixel_size_hr)\n",
    "  \n",
    "\n",
    "  # Generate random position for the top left corner of the patch\n",
    "  xc = np.random.randint(0, Mhr-patch_size, size=num_patches_per_frame)\n",
    "  yc = np.random.randint(0, Nhr-patch_size, size=num_patches_per_frame)\n",
    "\n",
    "  for c in range(len(xc)):\n",
    "    if LocImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size].sum() < min_number_of_emitters_per_patch:\n",
    "      skip_counter += 1\n",
    "      continue\n",
    "    \n",
    "    else:\n",
    "        # Limit maximal number of training examples to 15k\n",
    "      if k > max_num_patches:\n",
    "        break\n",
    "      else:\n",
    "        # Assign the patches to the right part of the images\n",
    "        patches[k-1] = upsampledFrame[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n",
    "        spikes[k-1] = LocImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n",
    "        heatmaps[k-1] = HeatMapImage[xc[c]:xc[c]+patch_size, yc[c]:yc[c]+patch_size]\n",
    "        k += 1 # increment current patch count\n",
    "\n",
    "# Remove the empty data\n",
    "patches = patches[:k-1]\n",
    "spikes = spikes[:k-1]\n",
    "heatmaps = heatmaps[:k-1]\n",
    "n_patches = k-1\n",
    "\n",
    "\n",
    "# -------------------- Failsafe --------------------\n",
    "# Check if the size of the training set is smaller than 5k to notify user to simulate more images using ThunderSTORM\n",
    "if ((k-1) < 5000):\n",
    "  # W  = '\\033[0m'  # white (normal)\n",
    "  # R  = '\\033[31m' # red\n",
    "  print(bcolors.WARNING+'!! WARNING: Training set size is below 5K - Consider simulating more images in ThunderSTORM. !!'+bcolors.NORMAL)\n",
    "\n",
    "\n",
    "# -------------------- Displays --------------------\n",
    "print('Number of patches skipped due to low density: '+str(skip_counter))\n",
    "# dataSize = int((getsizeof(patches)+getsizeof(heatmaps)+getsizeof(spikes))/(1024*1024)) #rounded in MB\n",
    "# print('Size of patches: '+str(dataSize)+' MB')\n",
    "print(str(n_patches)+' patches were generated.')\n",
    "\n",
    "# Displaying the time elapsed for training\n",
    "dt = time.time() - start\n",
    "minutes, seconds = divmod(dt, 60) \n",
    "hours, minutes = divmod(minutes, 60) \n",
    "print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n",
    "\n",
    "# Display patches interactively with a slider\n",
    "def scroll_patches(patch):\n",
    "  f = plt.figure(figsize=(16,6))\n",
    "  plt.subplot(1,3,1)\n",
    "  plt.imshow(patches[patch-1], interpolation='nearest', cmap='gray')\n",
    "  plt.title('Raw data (frame #'+str(patch)+')')\n",
    "  plt.axis('off');\n",
    "\n",
    "  plt.subplot(1,3,2)\n",
    "  plt.imshow(heatmaps[patch-1], interpolation='nearest')\n",
    "  plt.title('Heat map')\n",
    "  plt.axis('off');\n",
    "\n",
    "  plt.subplot(1,3,3)\n",
    "  plt.imshow(spikes[patch-1], interpolation='nearest')\n",
    "  plt.title('Localization map')\n",
    "  plt.axis('off');\n",
    "  \n",
    "  plt.savefig(path_to_fig,bbox_inches='tight',pad_inches=0)\n",
    "\n",
    "\n",
    "interact(scroll_patches, patch=widgets.IntSlider(min=1, max=patches.shape[0], step=1, value=0, continuous_update=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSjXFMevK7Iz"
   },
   "source": [
    "# **4. Train the network**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVeyKU0MdAPx"
   },
   "source": [
    "## **4.1. Select your paths and parameters**\n",
    "\n",
    "---\n",
    "\n",
    "<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
    "\n",
    "<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n",
    "\n",
    "\n",
    "<font size = 5>**Training parameters**\n",
    "\n",
    "<font size = 4>**`number_of_epochs`:**Input how many epochs (rounds) the network will be trained. Preliminary results can already be observed after a few (10-30) epochs, but a full training should run for ~100 epochs. Evaluate the performance after training (see 5). **Default value: 80**\n",
    "\n",
    "<font size =4>**`batch_size:`** This parameter defines the number of patches seen in each training step. Reducing or increasing the **batch size** may slow or speed up your training, respectively, and can influence network performance. **Default value: 16**\n",
    "\n",
    "<font size = 4>**`number_of_steps`:** Define the number of training steps by epoch. **If this value is set to 0**, by default this parameter is calculated so that each patch is seen at least once per epoch. **Default value: Number of patch / batch_size**\n",
    "\n",
    "<font size = 4>**`percentage_validation`:**  Input the percentage of your training dataset you want to use to validate the network during training. **Default value: 30** \n",
    "\n",
    "<font size = 4>**`initial_learning_rate`:** This parameter represents the initial value to be used as learning rate in the optimizer. **Default value: 0.001**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "oa5cDZ7f_PF6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps: 437\n",
      "-----------------------------\n",
      "Training parameters set.\n"
     ]
    }
   ],
   "source": [
    "#@markdown ###Path to training images and parameters\n",
    "\n",
    "######################################################################\n",
    "# ---------------------------- User input ----------------------------\n",
    "model_path = \"D:\\Data\\Anatomy\\KaarjelNara\\\\2021-03-10\" #@param {type: \"string\"} \n",
    "model_name = \"210310_testmodel\" #@param {type: \"string\"} \n",
    "number_of_epochs = 80 #@param {type:\"integer\"}\n",
    "batch_size =  16#@param {type:\"integer\"}\n",
    "\n",
    "number_of_steps =  0#@param {type:\"integer\"}\n",
    "percentage_validation = 30 #@param {type:\"number\"}\n",
    "initial_learning_rate = 0.001 #@param {type:\"number\"}\n",
    "######################################################################\n",
    "\n",
    "\n",
    "percentage_validation /= 100\n",
    "if number_of_steps == 0: \n",
    "  number_of_steps = int((1-percentage_validation)*n_patches/batch_size)\n",
    "  print('Number of steps: '+str(number_of_steps))\n",
    "\n",
    "# Pretrained model path initialised here so next cell does not need to be run\n",
    "h5_file_path = ''\n",
    "Use_pretrained_model = False\n",
    "\n",
    "if not ('patches' in locals()):\n",
    "  # W  = '\\033[0m'  # white (normal)\n",
    "  # R  = '\\033[31m' # red\n",
    "  print(WARNING+'!! WARNING: No patches were found in memory currently. !!')\n",
    "\n",
    "Save_path = os.path.join(model_path, model_name)\n",
    "if os.path.exists(Save_path):\n",
    "  print(bcolors.WARNING+'The model folder already exists and will be overwritten.'+bcolors.NORMAL)\n",
    "\n",
    "print('-----------------------------')\n",
    "print('Training parameters set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OADNcie-LHxA"
   },
   "source": [
    "## **4.4. Start Training**\n",
    "---\n",
    "<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n",
    "\n",
    "<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches.\n",
    "\n",
    "<font size = 4>Once training is complete, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder from Google Drive as all data can be erased at the next training if using the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "qDgMu_mAK8US"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "PDF report exported in D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10/210310_testmodel/\n",
      "Number of training examples: 7000\n",
      "Number of validation examples: 3000\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 208, 208, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF1 (Conv2D)          (None, 208, 208, 32)      288       \n",
      "_________________________________________________________________\n",
      "BN-CNNF1 (BatchNormalization (None, 208, 208, 32)      128       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF1 (Activation)      (None, 208, 208, 32)      0         \n",
      "_________________________________________________________________\n",
      "CNNPool1 (MaxPooling2D)      (None, 104, 104, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF2 (Conv2D)          (None, 104, 104, 64)      18432     \n",
      "_________________________________________________________________\n",
      "BN-CNNF2 (BatchNormalization (None, 104, 104, 64)      256       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF2 (Activation)      (None, 104, 104, 64)      0         \n",
      "_________________________________________________________________\n",
      "CNNPool2 (MaxPooling2D)      (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF3 (Conv2D)          (None, 52, 52, 128)       73728     \n",
      "_________________________________________________________________\n",
      "BN-CNNF3 (BatchNormalization (None, 52, 52, 128)       512       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF3 (Activation)      (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "CNNPool3 (MaxPooling2D)      (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF4 (Conv2D)          (None, 26, 26, 512)       589824    \n",
      "_________________________________________________________________\n",
      "BN-CNNF4 (BatchNormalization (None, 26, 26, 512)       2048      \n",
      "_________________________________________________________________\n",
      "Relu-CNNF4 (Activation)      (None, 26, 26, 512)       0         \n",
      "_________________________________________________________________\n",
      "CNNUpsample1 (UpSampling2D)  (None, 52, 52, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF5 (Conv2D)          (None, 52, 52, 128)       589824    \n",
      "_________________________________________________________________\n",
      "BN-CNNF5 (BatchNormalization (None, 52, 52, 128)       512       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF5 (Activation)      (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "CNNUpsample2 (UpSampling2D)  (None, 104, 104, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF6 (Conv2D)          (None, 104, 104, 64)      73728     \n",
      "_________________________________________________________________\n",
      "BN-CNNF6 (BatchNormalization (None, 104, 104, 64)      256       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF6 (Activation)      (None, 104, 104, 64)      0         \n",
      "_________________________________________________________________\n",
      "CNNUpsample3 (UpSampling2D)  (None, 208, 208, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv-CNNF7 (Conv2D)          (None, 208, 208, 32)      18432     \n",
      "_________________________________________________________________\n",
      "BN-CNNF7 (BatchNormalization (None, 208, 208, 32)      128       \n",
      "_________________________________________________________________\n",
      "Relu-CNNF7 (Activation)      (None, 208, 208, 32)      0         \n",
      "_________________________________________________________________\n",
      "Prediction (Conv2D)          (None, 208, 208, 1)       32        \n",
      "=================================================================\n",
      "Total params: 1,368,128\n",
      "Trainable params: 1,366,208\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "Using random initial model weights.\n",
      "-------------------------------\n",
      "Training model...\n",
      "Epoch 1/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.3856\n",
      "Epoch 00001: val_loss improved from inf to 0.51228, saving model to D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10\\210310_testmodel\\weights_best.hdf5\n",
      "437/437 [==============================] - 65s 149ms/step - loss: 0.3853 - val_loss: 0.5123\n",
      "Epoch 2/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.2454\n",
      "Epoch 00002: val_loss improved from 0.51228 to 0.47422, saving model to D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10\\210310_testmodel\\weights_best.hdf5\n",
      "437/437 [==============================] - 60s 137ms/step - loss: 0.2454 - val_loss: 0.4742\n",
      "Epoch 3/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.2048\n",
      "Epoch 00003: val_loss improved from 0.47422 to 0.34798, saving model to D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10\\210310_testmodel\\weights_best.hdf5\n",
      "437/437 [==============================] - 60s 137ms/step - loss: 0.2050 - val_loss: 0.3480\n",
      "Epoch 4/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.1823\n",
      "Epoch 00004: val_loss improved from 0.34798 to 0.17178, saving model to D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10\\210310_testmodel\\weights_best.hdf5\n",
      "437/437 [==============================] - 60s 138ms/step - loss: 0.1823 - val_loss: 0.1718\n",
      "Epoch 5/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.1685\n",
      "Epoch 00005: val_loss did not improve from 0.17178\n",
      "437/437 [==============================] - 60s 136ms/step - loss: 0.1684 - val_loss: 0.1994\n",
      "Epoch 6/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.1590\n",
      "Epoch 00006: val_loss did not improve from 0.17178\n",
      "437/437 [==============================] - 60s 137ms/step - loss: 0.1589 - val_loss: 0.5522\n",
      "Epoch 7/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.1532\n",
      "Epoch 00007: val_loss did not improve from 0.17178\n",
      "437/437 [==============================] - 60s 136ms/step - loss: 0.1533 - val_loss: 1.0703\n",
      "Epoch 8/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.1460\n",
      "Epoch 00008: val_loss did not improve from 0.17178\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.1459 - val_loss: 0.8543\n",
      "Epoch 9/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.1393\n",
      "Epoch 00009: val_loss did not improve from 0.17178\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.1392 - val_loss: 0.2237\n",
      "Epoch 10/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00010: val_loss improved from 0.17178 to 0.11891, saving model to D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10\\210310_testmodel\\weights_best.hdf5\n",
      "437/437 [==============================] - 59s 136ms/step - loss: 0.1084 - val_loss: 0.1189\n",
      "Epoch 11/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.1021\n",
      "Epoch 00011: val_loss improved from 0.11891 to 0.11652, saving model to D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10\\210310_testmodel\\weights_best.hdf5\n",
      "437/437 [==============================] - 59s 136ms/step - loss: 0.1021 - val_loss: 0.1165\n",
      "Epoch 12/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.1004\n",
      "Epoch 00012: val_loss did not improve from 0.11652\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.1004 - val_loss: 0.1219\n",
      "Epoch 13/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0977\n",
      "Epoch 00013: val_loss did not improve from 0.11652\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0977 - val_loss: 0.1166\n",
      "Epoch 14/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0968\n",
      "Epoch 00014: val_loss did not improve from 0.11652\n",
      "437/437 [==============================] - 59s 136ms/step - loss: 0.0967 - val_loss: 0.1170\n",
      "Epoch 15/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0943\n",
      "Epoch 00015: val_loss did not improve from 0.11652\n",
      "437/437 [==============================] - 60s 137ms/step - loss: 0.0943 - val_loss: 0.1174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0923\n",
      "Epoch 00016: val_loss improved from 0.11652 to 0.11398, saving model to D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10\\210310_testmodel\\weights_best.hdf5\n",
      "437/437 [==============================] - 60s 136ms/step - loss: 0.0923 - val_loss: 0.1140\n",
      "Epoch 17/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0907\n",
      "Epoch 00017: val_loss did not improve from 0.11398\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0906 - val_loss: 0.1227\n",
      "Epoch 18/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0884\n",
      "Epoch 00018: val_loss did not improve from 0.11398\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0884 - val_loss: 0.1148\n",
      "Epoch 19/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0866\n",
      "Epoch 00019: val_loss did not improve from 0.11398\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0865 - val_loss: 0.1208\n",
      "Epoch 20/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0849\n",
      "Epoch 00020: val_loss did not improve from 0.11398\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0849 - val_loss: 0.1171\n",
      "Epoch 21/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0831\n",
      "Epoch 00021: val_loss did not improve from 0.11398\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0831 - val_loss: 0.1158\n",
      "Epoch 22/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0774\n",
      "Epoch 00022: val_loss did not improve from 0.11398\n",
      "437/437 [==============================] - 59s 134ms/step - loss: 0.0774 - val_loss: 0.1213\n",
      "Epoch 23/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0770\n",
      "Epoch 00023: val_loss did not improve from 0.11398\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0770 - val_loss: 0.1143\n",
      "Epoch 24/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0750\n",
      "Epoch 00024: val_loss did not improve from 0.11398\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0750 - val_loss: 0.1158\n",
      "Epoch 25/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0741\n",
      "Epoch 00025: val_loss did not improve from 0.11398\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0741 - val_loss: 0.1140\n",
      "Epoch 26/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0722\n",
      "Epoch 00026: val_loss improved from 0.11398 to 0.11303, saving model to D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10\\210310_testmodel\\weights_best.hdf5\n",
      "437/437 [==============================] - 60s 136ms/step - loss: 0.0723 - val_loss: 0.1130\n",
      "Epoch 27/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0723\n",
      "Epoch 00027: val_loss improved from 0.11303 to 0.11301, saving model to D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10\\210310_testmodel\\weights_best.hdf5\n",
      "437/437 [==============================] - 59s 136ms/step - loss: 0.0723 - val_loss: 0.1130\n",
      "Epoch 28/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0705\n",
      "Epoch 00028: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0706 - val_loss: 0.1142\n",
      "Epoch 29/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0690\n",
      "Epoch 00029: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 60s 137ms/step - loss: 0.0690 - val_loss: 0.1142\n",
      "Epoch 30/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0678\n",
      "Epoch 00030: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 136ms/step - loss: 0.0677 - val_loss: 0.1132\n",
      "Epoch 31/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0669\n",
      "Epoch 00031: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 134ms/step - loss: 0.0668 - val_loss: 0.1134\n",
      "Epoch 32/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0665\n",
      "Epoch 00032: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0665 - val_loss: 0.1146\n",
      "Epoch 33/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0643\n",
      "Epoch 00033: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0643 - val_loss: 0.1145\n",
      "Epoch 34/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0641\n",
      "Epoch 00034: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0641 - val_loss: 0.1220\n",
      "Epoch 35/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0636\n",
      "Epoch 00035: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0636 - val_loss: 0.1165\n",
      "Epoch 36/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0617\n",
      "Epoch 00036: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 136ms/step - loss: 0.0617 - val_loss: 0.1312\n",
      "Epoch 37/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0616\n",
      "Epoch 00037: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0616 - val_loss: 0.1454\n",
      "Epoch 38/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0596\n",
      "Epoch 00038: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0596 - val_loss: 0.1163\n",
      "Epoch 39/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0592\n",
      "Epoch 00039: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0592 - val_loss: 0.1150\n",
      "Epoch 40/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0574\n",
      "Epoch 00040: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0574 - val_loss: 0.1274\n",
      "Epoch 41/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0574\n",
      "Epoch 00041: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 60s 136ms/step - loss: 0.0574 - val_loss: 0.1150\n",
      "Epoch 42/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0560\n",
      "Epoch 00042: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0561 - val_loss: 0.1189\n",
      "Epoch 43/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0552\n",
      "Epoch 00043: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0551 - val_loss: 0.1179\n",
      "Epoch 44/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0548\n",
      "Epoch 00044: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0548 - val_loss: 0.1171\n",
      "Epoch 45/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0535\n",
      "Epoch 00045: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0535 - val_loss: 0.1164\n",
      "Epoch 46/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0532\n",
      "Epoch 00046: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0532 - val_loss: 0.1156\n",
      "Epoch 47/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0519\n",
      "Epoch 00047: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0519 - val_loss: 0.1159\n",
      "Epoch 48/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0523\n",
      "Epoch 00048: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0522 - val_loss: 0.1185\n",
      "Epoch 49/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0502\n",
      "Epoch 00049: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0502 - val_loss: 0.1201\n",
      "Epoch 50/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0499\n",
      "Epoch 00050: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 134ms/step - loss: 0.0499 - val_loss: 0.1169\n",
      "Epoch 51/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0492\n",
      "Epoch 00051: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0492 - val_loss: 0.1543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0487\n",
      "Epoch 00052: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 134ms/step - loss: 0.0487 - val_loss: 0.1186\n",
      "Epoch 53/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0473\n",
      "Epoch 00053: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 134ms/step - loss: 0.0473 - val_loss: 0.1167\n",
      "Epoch 54/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0468\n",
      "Epoch 00054: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0468 - val_loss: 0.1171\n",
      "Epoch 55/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0459\n",
      "Epoch 00055: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0459 - val_loss: 0.1226\n",
      "Epoch 56/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0459\n",
      "Epoch 00056: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 61s 139ms/step - loss: 0.0460 - val_loss: 0.1522\n",
      "Epoch 57/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0458\n",
      "Epoch 00057: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 60s 137ms/step - loss: 0.0458 - val_loss: 0.1182\n",
      "Epoch 58/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0442\n",
      "Epoch 00058: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0442 - val_loss: 0.1184\n",
      "Epoch 59/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0436\n",
      "Epoch 00059: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0435 - val_loss: 0.1178\n",
      "Epoch 60/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0432\n",
      "Epoch 00060: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 136ms/step - loss: 0.0432 - val_loss: 0.1197\n",
      "Epoch 61/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0424\n",
      "Epoch 00061: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0424 - val_loss: 0.1213\n",
      "Epoch 62/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0424\n",
      "Epoch 00062: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0424 - val_loss: 0.1198\n",
      "Epoch 63/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0409\n",
      "Epoch 00063: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0409 - val_loss: 0.1286\n",
      "Epoch 64/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0405\n",
      "Epoch 00064: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0405 - val_loss: 0.1200\n",
      "Epoch 65/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0402\n",
      "Epoch 00065: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0402 - val_loss: 0.1276\n",
      "Epoch 66/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0401\n",
      "Epoch 00066: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0401 - val_loss: 0.1421\n",
      "Epoch 67/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0392\n",
      "Epoch 00067: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0392 - val_loss: 0.1271\n",
      "Epoch 68/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0387\n",
      "Epoch 00068: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0387 - val_loss: 0.1212\n",
      "Epoch 69/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0381\n",
      "Epoch 00069: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0381 - val_loss: 0.1197\n",
      "Epoch 70/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0376\n",
      "Epoch 00070: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0376 - val_loss: 0.1279\n",
      "Epoch 71/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0374\n",
      "Epoch 00071: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0374 - val_loss: 0.1240\n",
      "Epoch 72/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0371\n",
      "Epoch 00072: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0371 - val_loss: 0.1269\n",
      "Epoch 73/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0362\n",
      "Epoch 00073: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 136ms/step - loss: 0.0362 - val_loss: 0.1325\n",
      "Epoch 74/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0356\n",
      "Epoch 00074: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0356 - val_loss: 0.1197\n",
      "Epoch 75/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0356\n",
      "Epoch 00075: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0356 - val_loss: 0.1329\n",
      "Epoch 76/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0353\n",
      "Epoch 00076: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0353 - val_loss: 0.1201\n",
      "Epoch 77/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0343\n",
      "Epoch 00077: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 136ms/step - loss: 0.0343 - val_loss: 0.1226\n",
      "Epoch 78/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0338\n",
      "Epoch 00078: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0338 - val_loss: 0.1202\n",
      "Epoch 79/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0344\n",
      "Epoch 00079: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0344 - val_loss: 0.1219\n",
      "Epoch 80/80\n",
      "436/437 [============================>.] - ETA: 0s - loss: 0.0337\n",
      "Epoch 00080: val_loss did not improve from 0.11301\n",
      "437/437 [==============================] - 59s 135ms/step - loss: 0.0337 - val_loss: 0.1217\n",
      "-------------------------------\n",
      "Training Complete!\n",
      "Time elapsed: 1.0 hour(s) 19.0 min(s) 4 sec(s)\n",
      "------------------------------\n",
      "PDF report exported in D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10/210310_testmodel/\n"
     ]
    }
   ],
   "source": [
    "#@markdown ##Start training\n",
    "\n",
    "# Start the clock to measure how long it takes\n",
    "start = time.time()\n",
    "\n",
    "# --------------------- Using pretrained model ------------------------\n",
    "#Here we ensure that the learning rate set correctly when using pre-trained models\n",
    "if Use_pretrained_model:\n",
    "  if Weights_choice == \"last\":\n",
    "    initial_learning_rate = lastLearningRate\n",
    "\n",
    "  if Weights_choice == \"best\":            \n",
    "    initial_learning_rate = bestLearningRate\n",
    "# --------------------- ---------------------- ------------------------\n",
    "\n",
    "\n",
    "#here we check that no model with the same name already exist, if so delete\n",
    "if os.path.exists(Save_path):\n",
    "  shutil.rmtree(Save_path)\n",
    "\n",
    "# Create the model folder!\n",
    "os.makedirs(Save_path)\n",
    "\n",
    "# Export pdf summary \n",
    "pdf_export(raw_data = load_raw_data, pretrained_model = Use_pretrained_model)\n",
    "\n",
    "# Let's go!\n",
    "train_model(patches, heatmaps, Save_path, \n",
    "            steps_per_epoch=number_of_steps, epochs=number_of_epochs, batch_size=batch_size,\n",
    "            upsampling_factor = upsampling_factor,\n",
    "            validation_split = percentage_validation,\n",
    "            initial_learning_rate = initial_learning_rate, \n",
    "            pretrained_model_path = h5_file_path,\n",
    "            L2_weighting_factor = L2_weighting_factor)\n",
    "\n",
    "# Displaying the time elapsed for training\n",
    "dt = time.time() - start\n",
    "minutes, seconds = divmod(dt, 60) \n",
    "hours, minutes = divmod(minutes, 60) \n",
    "print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n",
    "\n",
    "# export pdf after training to update the existing document\n",
    "pdf_export(trained = True, raw_data = load_raw_data, pretrained_model = Use_pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4N7-ShZpLhwr"
   },
   "source": [
    "# **5. Evaluate your model**\n",
    "---\n",
    "\n",
    "<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model. \n",
    "\n",
    "<font size = 4>**We highly recommend to perform quality control on all newly trained models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "JDRsm7uKoBa-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 210310_testmodel model will be evaluated\n"
     ]
    }
   ],
   "source": [
    "# model name and path\n",
    "#@markdown ###Do you want to assess the model you just trained ?\n",
    "######################################################################\n",
    "# ---------------------------- User input ----------------------------\n",
    "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ###If not, please provide the path to the model folder:\n",
    "#@markdown #####During training, the model files are automatically saved inside a folder named after the parameter `model_name` (see section 4.1). Provide the name of this folder as `QC_model_path` . \n",
    "\n",
    "QC_model_path = \"\" #@param {type:\"string\"}\n",
    "######################################################################\n",
    "\n",
    "if (Use_the_current_trained_model): \n",
    "  QC_model_path = os.path.join(model_path, model_name)\n",
    "\n",
    "if os.path.exists(QC_model_path):\n",
    "  print(\"The \"+os.path.basename(QC_model_path)+\" model will be evaluated\")\n",
    "else:\n",
    "  print(bcolors.WARNING+'!! WARNING: The chosen model does not exist !!'+bcolors.NORMAL)\n",
    "  print('Please make sure you provide a valid model path before proceeding further.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gw7KaHZUoHC4"
   },
   "source": [
    "## **5.1. Inspection of the loss function**\n",
    "---\n",
    "\n",
    "<font size = 4>First, it is good practice to evaluate the training progress by comparing the training loss with the validation loss. The latter is a metric which shows how well the network performs on a subset of unseen data which is set aside from the training dataset. For more information on this, see for example [this review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6381354/) by Nichols *et al.*\n",
    "\n",
    "<font size = 4>**Training loss** describes an error value after each epoch for the difference between the model's prediction and its ground-truth target.\n",
    "\n",
    "<font size = 4>**Validation loss** describes the same error value between the model's prediction on a validation image and compared to it's target.\n",
    "\n",
    "<font size = 4>During training both values should decrease before reaching a minimal value which does not decrease further even after more training. Comparing the development of the validation loss with the training loss can give insights into the model's performance.\n",
    "\n",
    "<font size = 4>Decreasing **Training loss** and **Validation loss** indicates that training is still necessary and increasing the `number_of_epochs` is recommended. Note that the curves can look flat towards the right side, just because of the y-axis scaling. The network has reached convergence once the curves flatten out. After this point no further training is required. If the **Validation loss** suddenly increases again an the **Training loss** simultaneously goes towards zero, it means that the network is overfitting to the training data. In other words the network is remembering the exact patterns from the training data and no longer generalizes well to unseen data. In this case the training dataset has to be increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "id": "qUc-JMOcoGNZ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@markdown ##Play the cell to show a plot of training errors vs. epoch number\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "lossDataFromCSV = []\n",
    "vallossDataFromCSV = []\n",
    "\n",
    "with open(os.path.join(QC_model_path,'Quality Control/training_evaluation.csv'),'r') as csvfile:\n",
    "    csvRead = csv.reader(csvfile, delimiter=',')\n",
    "    next(csvRead)\n",
    "    for row in csvRead:\n",
    "        if row:\n",
    "            lossDataFromCSV.append(float(row[0]))\n",
    "            vallossDataFromCSV.append(float(row[1]))\n",
    "\n",
    "epochNumber = range(len(lossDataFromCSV))\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(epochNumber,lossDataFromCSV, label='Training loss')\n",
    "plt.plot(epochNumber,vallossDataFromCSV, label='Validation loss')\n",
    "plt.title('Training loss and validation loss vs. epoch number (linear scale)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.semilogy(epochNumber,lossDataFromCSV, label='Training loss')\n",
    "plt.semilogy(epochNumber,vallossDataFromCSV, label='Validation loss')\n",
    "plt.title('Training loss and validation loss vs. epoch number (log scale)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(QC_model_path,'Quality Control/lossCurvePlots.png'), bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32eNQjFioQkY"
   },
   "source": [
    "## **5.2. Error mapping and quality metrics estimation**\n",
    "---\n",
    "\n",
    "<font size = 4>This section will display SSIM maps and RSE maps as well as calculating total SSIM, NRMSE and PSNR metrics for all the images provided in the \"QC_image_folder\" using teh corresponding localization data contained in \"QC_loc_folder\" !\n",
    "\n",
    "<font size = 4>**1. The SSIM (structural similarity) map** \n",
    "\n",
    "<font size = 4>The SSIM metric is used to evaluate whether two images contain the same structures. It is a normalized metric and an SSIM of 1 indicates a perfect similarity between two images. Therefore for SSIM, the closer to 1, the better. The SSIM maps are constructed by calculating the SSIM metric in each pixel by considering the surrounding structural similarity in the neighbourhood of that pixel (currently defined as window of 11 pixels and with Gaussian weighting of 1.5 pixel standard deviation, see our Wiki for more info). \n",
    "\n",
    "<font size=4>**mSSIM** is the SSIM value calculated across the entire window of both images.\n",
    "\n",
    "<font size=4>**The output below shows the SSIM maps with the mSSIM**\n",
    "\n",
    "<font size = 4>**2. The RSE (Root Squared Error) map** \n",
    "\n",
    "<font size = 4>This is a display of the root of the squared difference between the normalized predicted and target or the source and the target. In this case, a smaller RSE is better. A perfect agreement between target and prediction will lead to an RSE map showing zeros everywhere (dark).\n",
    "\n",
    "\n",
    "<font size =4>**NRMSE (normalised root mean squared error)** gives the average difference between all pixels in the images compared to each other. Good agreement yields low NRMSE scores.\n",
    "\n",
    "<font size = 4>**PSNR (Peak signal-to-noise ratio)** is a metric that gives the difference between the ground truth and prediction (or source input) in decibels, using the peak pixel values of the prediction and the MSE between the images. The higher the score the better the agreement.\n",
    "\n",
    "<font size=4>**The output below shows the RSE maps with the NRMSE and PSNR values.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data\\Anatomy\\KaarjelNara\\2021-03-10\\210310_testmodel\\Quality Control\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU Memory\n",
    "tf.compat.v1.reset_default_graph()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "print(savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "form",
    "id": "dhlTnxC5lUZy",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "QC_TOM20_A.tif\n",
      "QC_TOM20_A_locs_conv.csv\n",
      "Input image is 128x128 with 2000 frames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                         | 0/2000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-0a6c00d352f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# Get the prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mbatchFramePredictionLocalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQC_image_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimageFilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQC_model_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msavePath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpixel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpixel_size_INPUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# test_model(QC_image_folder, imageFilename, QC_model_path, savePath, display=False);\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-a68cba484526>\u001b[0m in \u001b[0;36mbatchFramePredictionLocalization\u001b[1;34m(dataPath, filename, modelPath, savePath, batch_size, thresh, neighborhood_size, use_local_avg, pixel_size)\u001b[0m\n\u001b[0;32m    559\u001b[0m       \u001b[1;31m# Run prediction and local amxima finding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m       \u001b[0mpredicted_density\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImages_upsampled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m       \u001b[0mpredicted_density\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredicted_density\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m       \u001b[0mPrediction\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpredicted_density\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\carlo beretta\\.conda\\envs\\deep-storm\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__index__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__index__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    873\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Must be a NumPy scalar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# ---------------------------- User input ----------------------------\n",
    "#@markdown ##Choose the folders that contain your Quality Control dataset\n",
    "QC_image_folder = \"D:/Data/Anatomy/KaarjelNara//2021-03-10/validation/tif/\" #@param{type:\"string\"}\n",
    "QC_loc_folder = \"D:/Data/Anatomy/KaarjelNara//2021-03-10/validation/csv/\" #@param{type:\"string\"}\n",
    "#@markdown Get pixel size from file?\n",
    "get_pixel_size_from_file = False #@param {type:\"boolean\"}\n",
    "#@markdown Otherwise, use this value:\n",
    "pixel_size = 107 #@param {type:\"number\"}\n",
    "######################################################################\n",
    "\n",
    "if get_pixel_size_from_file:\n",
    "  pixel_size_INPUT = None\n",
    "else:\n",
    "  pixel_size_INPUT = pixel_size\n",
    "\n",
    "\n",
    "# ------------------------ QC analysis loop over provided dataset ------------------------\n",
    "\n",
    "savePath = os.path.join(QC_model_path, 'Quality Control')\n",
    "\n",
    "# Open and create the csv file that will contain all the QC metrics\n",
    "with open(os.path.join(savePath, os.path.basename(QC_model_path)+\"_QC_metrics.csv\"), \"w\", newline='') as file:\n",
    "  writer = csv.writer(file)\n",
    "\n",
    "  # Write the header in the csv file\n",
    "  writer.writerow([\"image #\",\"Prediction v. GT mSSIM\",\"WF v. GT mSSIM\", \"Prediction v. GT NRMSE\",\"WF v. GT NRMSE\", \"Prediction v. GT PSNR\", \"WF v. GT PSNR\"])\n",
    "\n",
    "  # These lists will be used to collect all the metrics values per slice\n",
    "  file_name_list = []\n",
    "  slice_number_list = []\n",
    "  mSSIM_GvP_list = []\n",
    "  mSSIM_GvWF_list = []\n",
    "  NRMSE_GvP_list = []\n",
    "  NRMSE_GvWF_list = []\n",
    "  PSNR_GvP_list = []\n",
    "  PSNR_GvWF_list = []\n",
    "\n",
    "  # Let's loop through the provided dataset in the QC folders\n",
    "\n",
    "  for (imageFilename, locFilename) in zip(list_files(QC_image_folder, 'tif'), list_files(QC_loc_folder, 'csv')):\n",
    "    print('--------------')\n",
    "    print(imageFilename)\n",
    "    print(locFilename)\n",
    "\n",
    "    # Get the prediction\n",
    "    batchFramePredictionLocalization(QC_image_folder, imageFilename, QC_model_path, savePath, pixel_size = pixel_size_INPUT)\n",
    "\n",
    "    # test_model(QC_image_folder, imageFilename, QC_model_path, savePath, display=False);\n",
    "    thisPrediction = io.imread(os.path.join(savePath, 'Predicted_'+imageFilename))\n",
    "    thisWidefield = io.imread(os.path.join(savePath, 'Widefield_'+imageFilename))\n",
    "\n",
    "    Mhr = thisPrediction.shape[0]\n",
    "    Nhr = thisPrediction.shape[1]\n",
    "\n",
    "    if pixel_size_INPUT == None:\n",
    "      pixel_size, N, M = getPixelSizeTIFFmetadata(os.path.join(QC_image_folder,imageFilename))\n",
    "\n",
    "    upsampling_factor = int(Mhr/M)\n",
    "    print('Upsampling factor: '+str(upsampling_factor))\n",
    "    pixel_size_hr = pixel_size/upsampling_factor # in nm\n",
    "\n",
    "    # Load the localization file and display the first\n",
    "    LocData = pd.read_csv(os.path.join(QC_loc_folder,locFilename), index_col=0)\n",
    "\n",
    "    x = np.array(list(LocData['x [nm]']))\n",
    "    y = np.array(list(LocData['y [nm]']))\n",
    "    locImage = FromLoc2Image_SimpleHistogram(x, y, image_size = (Mhr,Nhr), pixel_size = pixel_size_hr)\n",
    "\n",
    "    # Remove extension from filename\n",
    "    imageFilename_no_extension = os.path.splitext(imageFilename)[0]\n",
    "\n",
    "    # io.imsave(os.path.join(savePath, 'GT_image_'+imageFilename), locImage)\n",
    "    saveAsTIF(savePath, 'GT_image_'+imageFilename_no_extension, locImage, pixel_size_hr)\n",
    "\n",
    "    # Normalize the images wrt each other by minimizing the MSE between GT and prediction\n",
    "    test_GT_norm, test_prediction_norm = norm_minmse(locImage, thisPrediction, normalize_gt=True)\n",
    "    # Normalize the images wrt each other by minimizing the MSE between GT and Source image\n",
    "    test_GT_norm, test_wf_norm = norm_minmse(locImage, thisWidefield, normalize_gt=True)\n",
    "\n",
    "    \n",
    "    # -------------------------------- Calculate the metric maps and save them --------------------------------\n",
    "\n",
    "    # Calculate the SSIM maps\n",
    "    index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = structural_similarity(test_GT_norm, test_prediction_norm, data_range=1., full=True)\n",
    "    index_SSIM_GTvsWF, img_SSIM_GTvsWF = structural_similarity(test_GT_norm, test_wf_norm, data_range=1., full=True)\n",
    "\n",
    "\n",
    "    # Save ssim_maps\n",
    "    img_SSIM_GTvsPrediction_32bit = np.float32(img_SSIM_GTvsPrediction)\n",
    "    # io.imsave(os.path.join(savePath,'SSIM_GTvsPrediction_'+imageFilename),img_SSIM_GTvsPrediction_32bit)\n",
    "    saveAsTIF(savePath,'SSIM_GTvsPrediction_'+imageFilename_no_extension, img_SSIM_GTvsPrediction_32bit, pixel_size_hr)\n",
    "\n",
    "\n",
    "    img_SSIM_GTvsWF_32bit = np.float32(img_SSIM_GTvsWF)\n",
    "    # io.imsave(os.path.join(savePath,'SSIM_GTvsWF_'+imageFilename),img_SSIM_GTvsWF_32bit)\n",
    "    saveAsTIF(savePath,'SSIM_GTvsWF_'+imageFilename_no_extension, img_SSIM_GTvsWF_32bit, pixel_size_hr)\n",
    "\n",
    "  \n",
    "    # Calculate the Root Squared Error (RSE) maps\n",
    "    img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n",
    "    img_RSE_GTvsWF = np.sqrt(np.square(test_GT_norm - test_wf_norm))\n",
    "\n",
    "    # Save SE maps\n",
    "    img_RSE_GTvsPrediction_32bit = np.float32(img_RSE_GTvsPrediction)\n",
    "    # io.imsave(os.path.join(savePath,'RSE_GTvsPrediction_'+imageFilename),img_RSE_GTvsPrediction_32bit)\n",
    "    saveAsTIF(savePath,'RSE_GTvsPrediction_'+imageFilename_no_extension, img_RSE_GTvsPrediction_32bit, pixel_size_hr)\n",
    "\n",
    "    img_RSE_GTvsWF_32bit = np.float32(img_RSE_GTvsWF)\n",
    "    # io.imsave(os.path.join(savePath,'RSE_GTvsWF_'+imageFilename),img_RSE_GTvsWF_32bit)\n",
    "    saveAsTIF(savePath,'RSE_GTvsWF_'+imageFilename_no_extension, img_RSE_GTvsWF_32bit, pixel_size_hr)\n",
    "\n",
    "\n",
    "    # -------------------------------- Calculate the RSE metrics and save them --------------------------------\n",
    "\n",
    "    # Normalised Root Mean Squared Error (here it's valid to take the mean of the image)\n",
    "    NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n",
    "    NRMSE_GTvsWF = np.sqrt(np.mean(img_RSE_GTvsWF))\n",
    "    \n",
    "    # We can also measure the peak signal to noise ratio between the images\n",
    "    PSNR_GTvsPrediction = psnr(test_GT_norm,test_prediction_norm,data_range=1.0)\n",
    "    PSNR_GTvsWF = psnr(test_GT_norm,test_wf_norm,data_range=1.0)\n",
    "\n",
    "    writer.writerow([imageFilename,str(index_SSIM_GTvsPrediction),str(index_SSIM_GTvsWF),str(NRMSE_GTvsPrediction),str(NRMSE_GTvsWF),str(PSNR_GTvsPrediction), str(PSNR_GTvsWF)])\n",
    "\n",
    "    # Collect values to display in dataframe output\n",
    "    file_name_list.append(imageFilename)\n",
    "    mSSIM_GvP_list.append(index_SSIM_GTvsPrediction)\n",
    "    mSSIM_GvWF_list.append(index_SSIM_GTvsWF)\n",
    "    NRMSE_GvP_list.append(NRMSE_GTvsPrediction)\n",
    "    NRMSE_GvWF_list.append(NRMSE_GTvsWF)\n",
    "    PSNR_GvP_list.append(PSNR_GTvsPrediction)\n",
    "    PSNR_GvWF_list.append(PSNR_GTvsWF)\n",
    "\n",
    "\n",
    "# Table with metrics as dataframe output\n",
    "pdResults = pd.DataFrame(index = file_name_list)\n",
    "pdResults[\"Prediction v. GT mSSIM\"] = mSSIM_GvP_list\n",
    "pdResults[\"Wide-field v. GT mSSIM\"] = mSSIM_GvWF_list\n",
    "pdResults[\"Prediction v. GT NRMSE\"] = NRMSE_GvP_list\n",
    "pdResults[\"Wide-field v. GT NRMSE\"] = NRMSE_GvWF_list\n",
    "pdResults[\"Prediction v. GT PSNR\"] = PSNR_GvP_list\n",
    "pdResults[\"Wide-field v. GT PSNR\"] = PSNR_GvWF_list\n",
    "\n",
    "\n",
    "# ------------------------ Display ------------------------\n",
    "\n",
    "print('--------------------------------------------')\n",
    "@interact\n",
    "def show_QC_results(file = list_files(QC_image_folder, 'tif')):\n",
    "\n",
    "  plt.figure(figsize=(15,15))\n",
    "  # Target (Ground-truth)\n",
    "  plt.subplot(3,3,1)\n",
    "  plt.axis('off')\n",
    "  img_GT = io.imread(os.path.join(savePath, 'GT_image_'+file))\n",
    "  plt.imshow(img_GT, norm = simple_norm(img_GT, percent = 99.5))\n",
    "  plt.title('Target',fontsize=15)\n",
    "\n",
    "  # Wide-field\n",
    "  plt.subplot(3,3,2)\n",
    "  plt.axis('off')\n",
    "  img_Source = io.imread(os.path.join(savePath, 'Widefield_'+file))\n",
    "  plt.imshow(img_Source, norm = simple_norm(img_Source, percent = 99.5))\n",
    "  plt.title('Widefield',fontsize=15)\n",
    "\n",
    "  #Prediction\n",
    "  plt.subplot(3,3,3)\n",
    "  plt.axis('off')\n",
    "  img_Prediction = io.imread(os.path.join(savePath, 'Predicted_'+file))\n",
    "  plt.imshow(img_Prediction, norm = simple_norm(img_Prediction, percent = 99.5))\n",
    "  plt.title('Prediction',fontsize=15)\n",
    "\n",
    "  #Setting up colours\n",
    "  cmap = plt.cm.CMRmap\n",
    "\n",
    "  #SSIM between GT and Source\n",
    "  plt.subplot(3,3,5)\n",
    "  #plt.axis('off')\n",
    "  plt.tick_params(\n",
    "      axis='both',      # changes apply to the x-axis and y-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      bottom=False,      # ticks along the bottom edge are off\n",
    "      top=False,        # ticks along the top edge are off\n",
    "      left=False,       # ticks along the left edge are off\n",
    "      right=False,         # ticks along the right edge are off\n",
    "      labelbottom=False,\n",
    "      labelleft=False)\n",
    "  img_SSIM_GTvsWF = io.imread(os.path.join(savePath, 'SSIM_GTvsWF_'+file))\n",
    "  imSSIM_GTvsWF = plt.imshow(img_SSIM_GTvsWF, cmap = cmap, vmin=0, vmax=1)\n",
    "  plt.colorbar(imSSIM_GTvsWF,fraction=0.046, pad=0.04)\n",
    "  plt.title('Target vs. Widefield',fontsize=15)\n",
    "  plt.xlabel('mSSIM: '+str(round(pdResults.loc[file][\"Wide-field v. GT mSSIM\"],3)),fontsize=14)\n",
    "  plt.ylabel('SSIM maps',fontsize=20, rotation=0, labelpad=75)\n",
    "\n",
    "  #SSIM between GT and Prediction\n",
    "  plt.subplot(3,3,6)\n",
    "  #plt.axis('off')\n",
    "  plt.tick_params(\n",
    "      axis='both',      # changes apply to the x-axis and y-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      bottom=False,      # ticks along the bottom edge are off\n",
    "      top=False,        # ticks along the top edge are off\n",
    "      left=False,       # ticks along the left edge are off\n",
    "      right=False,         # ticks along the right edge are off\n",
    "      labelbottom=False,\n",
    "      labelleft=False)\n",
    "  img_SSIM_GTvsPrediction = io.imread(os.path.join(savePath, 'SSIM_GTvsPrediction_'+file))\n",
    "  imSSIM_GTvsPrediction = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n",
    "  plt.colorbar(imSSIM_GTvsPrediction,fraction=0.046, pad=0.04)\n",
    "  plt.title('Target vs. Prediction',fontsize=15)\n",
    "  plt.xlabel('mSSIM: '+str(round(pdResults.loc[file][\"Prediction v. GT mSSIM\"],3)),fontsize=14)\n",
    "\n",
    "  #Root Squared Error between GT and Source\n",
    "  plt.subplot(3,3,8)\n",
    "  #plt.axis('off')\n",
    "  plt.tick_params(\n",
    "      axis='both',      # changes apply to the x-axis and y-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      bottom=False,      # ticks along the bottom edge are off\n",
    "      top=False,        # ticks along the top edge are off\n",
    "      left=False,       # ticks along the left edge are off\n",
    "      right=False,         # ticks along the right edge are off\n",
    "      labelbottom=False,\n",
    "      labelleft=False)\n",
    "  img_RSE_GTvsWF = io.imread(os.path.join(savePath, 'RSE_GTvsWF_'+file))\n",
    "  imRSE_GTvsWF = plt.imshow(img_RSE_GTvsWF, cmap = cmap, vmin=0, vmax = 1)\n",
    "  plt.colorbar(imRSE_GTvsWF,fraction=0.046,pad=0.04)\n",
    "  plt.title('Target vs. Widefield',fontsize=15)\n",
    "  plt.xlabel('NRMSE: '+str(round(pdResults.loc[file][\"Wide-field v. GT NRMSE\"],3))+', PSNR: '+str(round(pdResults.loc[file][\"Wide-field v. GT PSNR\"],3)),fontsize=14)\n",
    "  plt.ylabel('RSE maps',fontsize=20, rotation=0, labelpad=75)\n",
    "\n",
    "  #Root Squared Error between GT and Prediction\n",
    "  plt.subplot(3,3,9)\n",
    "  #plt.axis('off')\n",
    "  plt.tick_params(\n",
    "      axis='both',      # changes apply to the x-axis and y-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      bottom=False,      # ticks along the bottom edge are off\n",
    "      top=False,        # ticks along the top edge are off\n",
    "      left=False,       # ticks along the left edge are off\n",
    "      right=False,         # ticks along the right edge are off\n",
    "      labelbottom=False,\n",
    "      labelleft=False)\n",
    "  img_RSE_GTvsPrediction = io.imread(os.path.join(savePath, 'RSE_GTvsPrediction_'+file))\n",
    "  imRSE_GTvsPrediction = plt.imshow(img_RSE_GTvsPrediction, cmap = cmap, vmin=0, vmax=1)\n",
    "  plt.colorbar(imRSE_GTvsPrediction,fraction=0.046,pad=0.04)\n",
    "  plt.title('Target vs. Prediction',fontsize=15)\n",
    "  plt.xlabel('NRMSE: '+str(round(pdResults.loc[file][\"Prediction v. GT NRMSE\"],3))+', PSNR: '+str(round(pdResults.loc[file][\"Prediction v. GT PSNR\"],3)),fontsize=14)\n",
    "  plt.savefig(QC_model_path+'/Quality Control/QC_example_data.png', bbox_inches='tight', pad_inches=0)\n",
    "print('--------------------------------------------')\n",
    "pdResults.head()\n",
    "\n",
    "# Export pdf wth summary of QC results\n",
    "qc_pdf_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTRou0izLjhd"
   },
   "source": [
    "# **6. Using the trained model**\n",
    "\n",
    "---\n",
    "\n",
    "<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAf8aBDmWTx7"
   },
   "source": [
    "## **6.1 Generate image prediction and localizations from unseen dataset**\n",
    "---\n",
    "\n",
    "<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder as restored image stacks (ImageJ-compatible TIFF images).\n",
    "\n",
    "<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n",
    "\n",
    "<font size = 4>**`Result_folder`:** This folder will contain the found localizations csv.\n",
    "\n",
    "<font size = 4>**`batch_size`:** This paramter determines how many frames are processed by any single pass on the GPU. A higher `batch_size` will make the prediction faster but will use more GPU memory. If an OutOfMemory (OOM) error occurs, decrease the `batch_size`. **DEFAULT: 4**\n",
    "\n",
    "<font size = 4>**`threshold`:** This paramter determines threshold for local maxima finding. The value is expected to reside in the range **[0,1]**. A higher `threshold` will result in less localizations. **DEFAULT: 0.1**\n",
    "\n",
    "<font size = 4>**`neighborhood_size`:** This paramter determines size of the neighborhood within which the prediction needs to be a local maxima in recovery pixels (CCD pixel/upsampling_factor). A high `neighborhood_size` will make the prediction slower and potentially discard nearby localizations. **DEFAULT: 3**\n",
    "\n",
    "<font size = 4>**`use_local_average`:** This paramter determines whether to locally average the prediction in a 3x3 neighborhood to get the final localizations. If set to **True** it will make inference slightly slower depending on the size of the FOV. **DEFAULT: True**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7qn06T_A0lxf"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# ---------------------------- User input ----------------------------\n",
    "#@markdown ### Data parameters\n",
    "Data_folder = \"\" #@param {type:\"string\"}\n",
    "Result_folder = \"\" #@param {type:\"string\"}\n",
    "#@markdown Get pixel size from file?\n",
    "get_pixel_size_from_file = True #@param {type:\"boolean\"}\n",
    "#@markdown Otherwise, use this value (in nm):\n",
    "pixel_size = 100 #@param {type:\"number\"}\n",
    "\n",
    "#@markdown ### Model parameters\n",
    "#@markdown Do you want to use the model you just trained?\n",
    "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
    "#@markdown Otherwise, please provide path to the model folder below\n",
    "prediction_model_path = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Prediction parameters\n",
    "batch_size =  4#@param {type:\"integer\"}\n",
    "\n",
    "#@markdown ### Post processing parameters\n",
    "threshold =  0.1#@param {type:\"number\"}\n",
    "neighborhood_size =  3#@param {type:\"integer\"}\n",
    "#@markdown Do you want to locally average the model output with CoG estimator ?\n",
    "use_local_average = True #@param {type:\"boolean\"}\n",
    "######################################################################\n",
    "\n",
    "\n",
    "if get_pixel_size_from_file:\n",
    "  pixel_size = None\n",
    "\n",
    "if (Use_the_current_trained_model): \n",
    "  prediction_model_path = os.path.join(model_path, model_name)\n",
    "\n",
    "if os.path.exists(prediction_model_path):\n",
    "  print(\"The \"+os.path.basename(prediction_model_path)+\" model will be used.\")\n",
    "else:\n",
    "  print(bcolors.WARNING+'!! WARNING: The chosen model does not exist !!'+bcolors.NORMAL)\n",
    "  print('Please make sure you provide a valid model path before proceeding further.')\n",
    "\n",
    "# inform user whether local averaging is being used\n",
    "if use_local_average == True: \n",
    "  print('Using local averaging')\n",
    "\n",
    "if not os.path.exists(Result_folder):\n",
    "  print('Result folder was created.')\n",
    "  os.makedirs(Result_folder)\n",
    "\n",
    "\n",
    "# ------------------------------- Run predictions -------------------------------\n",
    "\n",
    "start = time.time()\n",
    "#%% This script tests the trained fully convolutional network based on the \n",
    "# saved training weights, and normalization created using train_model.\n",
    "\n",
    "if os.path.isdir(Data_folder): \n",
    "  for filename in list_files(Data_folder, 'tif'):\n",
    "    # run the testing/reconstruction process\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Running prediction on: \"+ filename)\n",
    "    batchFramePredictionLocalization(Data_folder, filename, prediction_model_path, Result_folder, \n",
    "                                     batch_size, \n",
    "                                     threshold, \n",
    "                                     neighborhood_size, \n",
    "                                     use_local_average,\n",
    "                                     pixel_size = pixel_size)\n",
    "\n",
    "elif os.path.isfile(Data_folder):\n",
    "  batchFramePredictionLocalization(os.path.dirname(Data_folder), os.path.basename(Data_folder), prediction_model_path, Result_folder, \n",
    "                                   batch_size, \n",
    "                                   threshold, \n",
    "                                   neighborhood_size, \n",
    "                                   use_local_average, \n",
    "                                   pixel_size = pixel_size)\n",
    "\n",
    "\n",
    "print('--------------------------------------------------------------------')\n",
    "# Displaying the time elapsed for training\n",
    "dt = time.time() - start\n",
    "minutes, seconds = divmod(dt, 60) \n",
    "hours, minutes = divmod(minutes, 60) \n",
    "print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n",
    "\n",
    "\n",
    "# ------------------------------- Interactive display -------------------------------\n",
    "\n",
    "print('--------------------------------------------------------------------')\n",
    "print('---------------------------- Previews ------------------------------')\n",
    "print('--------------------------------------------------------------------')\n",
    "\n",
    "if os.path.isdir(Data_folder): \n",
    "  @interact\n",
    "  def show_QC_results(file = list_files(Data_folder, 'tif')):\n",
    "\n",
    "    plt.figure(figsize=(15,7.5))\n",
    "    # Wide-field\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    img_Source = io.imread(os.path.join(Result_folder, 'Widefield_'+file))\n",
    "    plt.imshow(img_Source, norm = simple_norm(img_Source, percent = 99.5))\n",
    "    plt.title('Widefield', fontsize=15)\n",
    "    # Prediction\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    img_Prediction = io.imread(os.path.join(Result_folder, 'Predicted_'+file))\n",
    "    plt.imshow(img_Prediction, norm = simple_norm(img_Prediction, percent = 99.5))\n",
    "    plt.title('Predicted',fontsize=15)\n",
    "\n",
    "if os.path.isfile(Data_folder):\n",
    "\n",
    "  plt.figure(figsize=(15,7.5))\n",
    "  # Wide-field\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.axis('off')\n",
    "  img_Source = io.imread(os.path.join(Result_folder, 'Widefield_'+os.path.basename(Data_folder)))\n",
    "  plt.imshow(img_Source, norm = simple_norm(img_Source, percent = 99.5))\n",
    "  plt.title('Widefield', fontsize=15)\n",
    "  # Prediction\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.axis('off')\n",
    "  img_Prediction = io.imread(os.path.join(Result_folder, 'Predicted_'+os.path.basename(Data_folder)))\n",
    "  plt.imshow(img_Prediction, norm = simple_norm(img_Prediction, percent = 99.5))\n",
    "  plt.title('Predicted',fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZekzexaPmzFZ"
   },
   "source": [
    "## **6.2 Drift correction**\n",
    "---\n",
    "\n",
    "<font size = 4>The visualization above is the raw output of the network and displayed at the `upsampling_factor` chosen during model training. The display is a preview without any drift correction applied. This section performs drift correction using cross-correlation between time bins to estimate the drift.\n",
    "\n",
    "<font size = 4>**`Loc_file_path`:** is the path to the localization file to use for visualization.\n",
    "\n",
    "<font size = 4>**`original_image_path`:** is the path to the original image. This only serves to extract the original image size and pixel size to shape the visualization properly.\n",
    "\n",
    "<font size = 4>**`visualization_pixel_size`:** This parameter corresponds to the pixel size to use for the image reconstructions used for the Drift Correction estmication (in **nm**). A smaller pixel size will be more precise but will take longer to compute. **DEFAULT: 20**\n",
    "\n",
    "<font size = 4>**`number_of_bins`:** This parameter defines how many temporal bins are used across the full dataset. All localizations in each bins are used ot build an image. This image is used to find the drift with respect to the image obtained from the very first bin. A typical value would correspond to about 500 frames per bin. **DEFAULT: Total number of frames / 500**\n",
    "\n",
    "<font size = 4>**`polynomial_fit_degree`:** The drift obtained for each temporal bins needs to be interpolated to every single frames. This is performed by polynomial fit, the degree of which is defined here. **DEFAULT: 4**\n",
    "\n",
    "<font size = 4> The drift-corrected localization data is automaticaly saved in the `save_path` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hYtP_vh6mzUP"
   },
   "outputs": [],
   "source": [
    "# @markdown ##Data parameters\n",
    "Loc_file_path = \"\" #@param {type:\"string\"}\n",
    "# @markdown Provide information about original data. Get the info automatically from the raw data?\n",
    "Get_info_from_file = True #@param {type:\"boolean\"}\n",
    "# Loc_file_path = \"/content/gdrive/My Drive/Colab notebooks testing/DeepSTORM/Glia data from CL/Results from prediction/20200615-M6 with CoM localizations/Localizations_glia_actin_2D - 1-500fr_avg.csv\" #@param {type:\"string\"}\n",
    "original_image_path = \"\" #@param {type:\"string\"}\n",
    "# @markdown Otherwise, please provide image width, height (in pixels) and pixel size (in nm)\n",
    "image_width =  256#@param {type:\"integer\"}\n",
    "image_height =  256#@param {type:\"integer\"}\n",
    "pixel_size = 100 #@param {type:\"number\"}\n",
    "\n",
    "# @markdown ##Drift correction parameters\n",
    "visualization_pixel_size =  20#@param {type:\"number\"}\n",
    "number_of_bins =  50#@param {type:\"integer\"}\n",
    "polynomial_fit_degree =  4#@param {type:\"integer\"}\n",
    "\n",
    "# @markdown ##Saving parameters\n",
    "save_path = '' #@param {type:\"string\"}\n",
    "\n",
    "# Let's go !\n",
    "start = time.time()\n",
    "\n",
    "# Get info from the raw file if selected\n",
    "if Get_info_from_file:\n",
    "  pixel_size, image_width, image_height = getPixelSizeTIFFmetadata(original_image_path, display=True)\n",
    "\n",
    "# Read the localizations in\n",
    "LocData = pd.read_csv(Loc_file_path)\n",
    "\n",
    "# Calculate a few variables \n",
    "Mhr = int(math.ceil(image_height*pixel_size/visualization_pixel_size))\n",
    "Nhr = int(math.ceil(image_width*pixel_size/visualization_pixel_size))\n",
    "nFrames = max(LocData['frame'])\n",
    "x_max = max(LocData['x [nm]'])\n",
    "y_max = max(LocData['y [nm]'])\n",
    "image_size = (Mhr, Nhr)\n",
    "n_locs = len(LocData.index)\n",
    "\n",
    "print('Image size: '+str(image_size))\n",
    "print('Number of frames in data: '+str(nFrames))\n",
    "print('Number of localizations in data: '+str(n_locs))\n",
    "\n",
    "blocksize = math.ceil(nFrames/number_of_bins)\n",
    "print('Number of frames per block: '+str(blocksize))\n",
    "\n",
    "blockDataFrame = LocData[(LocData['frame'] < blocksize)].copy()\n",
    "xc_array = blockDataFrame['x [nm]'].to_numpy(dtype=np.float32)\n",
    "yc_array = blockDataFrame['y [nm]'].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Preparing the Reference image\n",
    "photon_array = np.ones(yc_array.shape[0])\n",
    "sigma_array = np.ones(yc_array.shape[0])\n",
    "ImageRef = FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = image_size, pixel_size = visualization_pixel_size)\n",
    "ImagesRef = np.rot90(ImageRef, k=2)\n",
    "\n",
    "xDrift = np.zeros(number_of_bins)\n",
    "yDrift = np.zeros(number_of_bins)\n",
    "\n",
    "filename_no_extension = os.path.splitext(os.path.basename(Loc_file_path))[0]\n",
    "\n",
    "with open(os.path.join(save_path, filename_no_extension+\"_DriftCorrectionData.csv\"), \"w\", newline='') as file:\n",
    "  writer = csv.writer(file)\n",
    "\n",
    "  # Write the header in the csv file\n",
    "  writer.writerow([\"Block #\", \"x-drift [nm]\",\"y-drift [nm]\"])\n",
    "\n",
    "  for b in tqdm(range(number_of_bins)):\n",
    "\n",
    "    blockDataFrame = LocData[(LocData['frame'] >= (b*blocksize)) & (LocData['frame'] < ((b+1)*blocksize))].copy()\n",
    "    xc_array = blockDataFrame['x [nm]'].to_numpy(dtype=np.float32)\n",
    "    yc_array = blockDataFrame['y [nm]'].to_numpy(dtype=np.float32)\n",
    "\n",
    "    photon_array = np.ones(yc_array.shape[0])\n",
    "    sigma_array = np.ones(yc_array.shape[0])\n",
    "    ImageBlock = FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = image_size, pixel_size = visualization_pixel_size)\n",
    "\n",
    "    XC = fftconvolve(ImagesRef, ImageBlock, mode = 'same')\n",
    "    yDrift[b], xDrift[b] = subPixelMaxLocalization(XC, method = 'CoM')\n",
    "\n",
    "    # saveAsTIF(save_path, 'ImageBlock'+str(b), ImageBlock, visualization_pixel_size)\n",
    "    # saveAsTIF(save_path, 'XCBlock'+str(b), XC, visualization_pixel_size)\n",
    "    writer.writerow([str(b), str((xDrift[b]-xDrift[0])*visualization_pixel_size), str((yDrift[b]-yDrift[0])*visualization_pixel_size)])\n",
    "\n",
    "print('--------------------------------------------------------------------')\n",
    "# Displaying the time elapsed for training\n",
    "dt = time.time() - start\n",
    "minutes, seconds = divmod(dt, 60) \n",
    "hours, minutes = divmod(minutes, 60) \n",
    "print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n",
    "\n",
    "print('Fitting drift data...')\n",
    "bin_number = np.arange(number_of_bins)*blocksize + blocksize/2\n",
    "xDrift = (xDrift-xDrift[0])*visualization_pixel_size\n",
    "yDrift = (yDrift-yDrift[0])*visualization_pixel_size\n",
    "\n",
    "xDriftCoeff = np.polyfit(bin_number, xDrift, polynomial_fit_degree)\n",
    "yDriftCoeff = np.polyfit(bin_number, yDrift, polynomial_fit_degree)\n",
    "\n",
    "xDriftFit = np.poly1d(xDriftCoeff)\n",
    "yDriftFit = np.poly1d(yDriftCoeff)\n",
    "bins = np.arange(nFrames)\n",
    "xDriftInterpolated = xDriftFit(bins)\n",
    "yDriftInterpolated = yDriftFit(bins)\n",
    "\n",
    "\n",
    "# ------------------ Displaying the image results ------------------\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(bin_number,xDrift, 'r+', label='x-drift')\n",
    "plt.plot(bin_number,yDrift, 'b+', label='y-drift')\n",
    "plt.plot(bins,xDriftInterpolated, 'r-', label='y-drift (fit)')\n",
    "plt.plot(bins,yDriftInterpolated, 'b-', label='y-drift (fit)')\n",
    "plt.title('Cross-correlation estimated drift')\n",
    "plt.ylabel('Drift [nm]')\n",
    "plt.xlabel('Bin number')\n",
    "plt.legend();\n",
    "\n",
    "dt = time.time() - start\n",
    "minutes, seconds = divmod(dt, 60) \n",
    "hours, minutes = divmod(minutes, 60) \n",
    "print(\"Time elapsed:\", hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n",
    "\n",
    "\n",
    "# ------------------ Actual drift correction -------------------\n",
    "\n",
    "print('Correcting localization data...')\n",
    "xc_array = LocData['x [nm]'].to_numpy(dtype=np.float32)\n",
    "yc_array = LocData['y [nm]'].to_numpy(dtype=np.float32)\n",
    "frames = LocData['frame'].to_numpy(dtype=np.int32)\n",
    "\n",
    "\n",
    "xc_array_Corr, yc_array_Corr = correctDriftLocalization(xc_array, yc_array, frames, xDriftInterpolated, yDriftInterpolated)\n",
    "ImageRaw = FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = image_size, pixel_size = visualization_pixel_size)\n",
    "ImageCorr = FromLoc2Image_SimpleHistogram(xc_array_Corr, yc_array_Corr, image_size = image_size, pixel_size = visualization_pixel_size)\n",
    "\n",
    "\n",
    "# ------------------ Displaying the imge results ------------------\n",
    "plt.figure(figsize=(15,7.5))\n",
    "# Raw\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis('off')\n",
    "plt.imshow(ImageRaw, norm = simple_norm(ImageRaw, percent = 99.5))\n",
    "plt.title('Raw', fontsize=15);\n",
    "# Corrected\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis('off')\n",
    "plt.imshow(ImageCorr, norm = simple_norm(ImageCorr, percent = 99.5))\n",
    "plt.title('Corrected',fontsize=15);\n",
    "\n",
    "\n",
    "# ------------------ Table with info -------------------\n",
    "driftCorrectedLocData = pd.DataFrame()\n",
    "driftCorrectedLocData['frame'] = frames\n",
    "driftCorrectedLocData['x [nm]'] = xc_array_Corr\n",
    "driftCorrectedLocData['y [nm]'] = yc_array_Corr\n",
    "driftCorrectedLocData['confidence [a.u]'] = LocData['confidence [a.u]']\n",
    "\n",
    "driftCorrectedLocData.to_csv(os.path.join(save_path, filename_no_extension+'_DriftCorrected.csv'))\n",
    "print('-------------------------------')\n",
    "print('Corrected localizations saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzOuc-V7rB-r"
   },
   "source": [
    "## **6.3 Visualization of the localizations**\n",
    "---\n",
    "\n",
    "\n",
    "<font size = 4>The visualization in section 6.1 is the raw output of the network and displayed at the `upsampling_factor` chosen during model training. This section performs visualization of the result by plotting the localizations as a simple histogram.\n",
    "\n",
    "<font size = 4>**`Loc_file_path`:** is the path to the localization file to use for visualization.\n",
    "\n",
    "<font size = 4>**`original_image_path`:** is the path to the original image. This only serves to extract the original image size and pixel size to shape the visualization properly.\n",
    "\n",
    "<font size = 4>**`visualization_pixel_size`:** This parameter corresponds to the pixel size to use for the final image reconstruction (in **nm**). **DEFAULT: 10**\n",
    "\n",
    "<font size = 4>**`visualization_mode`:** This parameter defines what visualization method is used to visualize the final image. NOTES: The Integrated Gaussian can be quite slow. **DEFAULT: Simple histogram.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "876yIXnqq-nW"
   },
   "outputs": [],
   "source": [
    "# @markdown ##Data parameters\n",
    "######################################################################\n",
    "# ---------------------------- User input ----------------------------\n",
    "Use_current_drift_corrected_localizations = True #@param {type:\"boolean\"}\n",
    "# @markdown Otherwise provide a localization file path\n",
    "Loc_file_path = \"\" #@param {type:\"string\"}\n",
    "# @markdown Provide information about original data. Get the info automatically from the raw data?\n",
    "Get_info_from_file = True #@param {type:\"boolean\"}\n",
    "# Loc_file_path = \"/content/gdrive/My Drive/Colab notebooks testing/DeepSTORM/Glia data from CL/Results from prediction/20200615-M6 with CoM localizations/Localizations_glia_actin_2D - 1-500fr_avg.csv\" #@param {type:\"string\"}\n",
    "original_image_path = \"\" #@param {type:\"string\"}\n",
    "# @markdown Otherwise, please provide image width, height (in pixels) and pixel size (in nm)\n",
    "image_width =  256#@param {type:\"integer\"}\n",
    "image_height =  256#@param {type:\"integer\"}\n",
    "pixel_size =  100#@param {type:\"number\"}\n",
    "\n",
    "# @markdown ##Visualization parameters\n",
    "visualization_pixel_size =  10#@param {type:\"number\"}\n",
    "visualization_mode = \"Simple histogram\" #@param [\"Simple histogram\", \"Integrated Gaussian (SLOW!)\"]\n",
    "######################################################################\n",
    "\n",
    "if not Use_current_drift_corrected_localizations:\n",
    "  filename_no_extension = os.path.splitext(os.path.basename(Loc_file_path))[0]\n",
    "\n",
    "\n",
    "if Get_info_from_file:\n",
    "  pixel_size, image_width, image_height = getPixelSizeTIFFmetadata(original_image_path, display=True)\n",
    "\n",
    "if Use_current_drift_corrected_localizations:\n",
    "  LocData = driftCorrectedLocData\n",
    "else:\n",
    "  LocData = pd.read_csv(Loc_file_path)\n",
    "\n",
    "Mhr = int(math.ceil(image_height*pixel_size/visualization_pixel_size))\n",
    "Nhr = int(math.ceil(image_width*pixel_size/visualization_pixel_size))\n",
    "\n",
    "\n",
    "nFrames = max(LocData['frame'])\n",
    "x_max = max(LocData['x [nm]'])\n",
    "y_max = max(LocData['y [nm]'])\n",
    "image_size = (Mhr, Nhr)\n",
    "\n",
    "print('Image size: '+str(image_size))\n",
    "print('Number of frames in data: '+str(nFrames))\n",
    "print('Number of localizations in data: '+str(len(LocData.index)))\n",
    "\n",
    "xc_array = LocData['x [nm]'].to_numpy()\n",
    "yc_array = LocData['y [nm]'].to_numpy()\n",
    "if (visualization_mode == 'Simple histogram'):\n",
    "  locImage = FromLoc2Image_SimpleHistogram(xc_array, yc_array, image_size = image_size, pixel_size = visualization_pixel_size)\n",
    "elif (visualization_mode == 'Shifted histogram'):\n",
    "  print(bcolors.WARNING+'Method not implemented yet!'+bcolors.NORMAL)\n",
    "  locImage = np.zeros(image_size)\n",
    "elif (visualization_mode == 'Integrated Gaussian (SLOW!)'):\n",
    "  photon_array = np.ones(xc_array.shape)\n",
    "  sigma_array = np.ones(xc_array.shape)\n",
    "  locImage = FromLoc2Image_Erf(xc_array, yc_array, photon_array, sigma_array, image_size = image_size, pixel_size = visualization_pixel_size)\n",
    "\n",
    "print('--------------------------------------------------------------------')\n",
    "# Displaying the time elapsed for training\n",
    "dt = time.time() - start\n",
    "minutes, seconds = divmod(dt, 60) \n",
    "hours, minutes = divmod(minutes, 60) \n",
    "print(\"Time elapsed:\",hours, \"hour(s)\",minutes,\"min(s)\",round(seconds),\"sec(s)\")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.axis('off')\n",
    "# plt.imshow(locImage, cmap='gray');\n",
    "plt.imshow(locImage, norm = simple_norm(locImage, percent = 99.5));\n",
    "\n",
    "\n",
    "LocData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "PdOhWwMn1zIT"
   },
   "outputs": [],
   "source": [
    "# @markdown ---\n",
    "# @markdown #Play this cell to save the visualization\n",
    "# @markdown ####Please select a path to the folder where to save the visualization.\n",
    "######################################################################\n",
    "# ---------------------------- User input ----------------------------\n",
    "save_path = \"\" #@param {type:\"string\"}\n",
    "######################################################################\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "  os.makedirs(save_path)\n",
    "  print('Folder created.')\n",
    "\n",
    "saveAsTIF(save_path, filename_no_extension+'_Visualization', locImage, visualization_pixel_size)\n",
    "print('Image saved.', save_path + '/' + filename_no_extension)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Deep-STORM_2D_ZeroCostDL4Mic.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
